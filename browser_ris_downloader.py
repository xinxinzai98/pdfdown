#!/usr/bin/env python3
"""
æµè§ˆå™¨ç‰ˆ RIS æ–‡ä»¶æ‰¹é‡ä¸‹è½½å™¨ - ä½¿ç”¨ Playwright æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ä¸‹è½½

ä¼˜åŠ¿ï¼š
1. ç»•è¿‡åçˆ¬è™«æ£€æµ‹
2. æ”¯æŒ JavaScript æ¸²æŸ“çš„é¡µé¢
3. å¯ä»¥æ‰‹åŠ¨é€šè¿‡éªŒè¯ï¼ˆäº¤äº’æ¨¡å¼ï¼‰
"""

import argparse
import asyncio
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from lib.sources.multi_channel_browser import MultiChannelBrowserDownloader
from lib.utils.report import HTMLReportGenerator

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def parse_ris_file(ris_path: str) -> List[Dict[str, str]]:
    """è§£æ RIS æ–‡ä»¶ï¼Œæå– DOI å’Œå…ƒæ•°æ®"""
    papers = []
    current_entry = {}

    with open(ris_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            if line.startswith("TY  -"):
                if current_entry and current_entry.get("doi"):
                    papers.append(current_entry)
                current_entry = {}
            elif line.startswith("DO  -"):
                current_entry["doi"] = line[5:].strip()
            elif line.startswith("TI  -"):
                current_entry["title"] = line[5:].strip()
            elif line.startswith("AU  -"):
                if "authors" not in current_entry:
                    current_entry["authors"] = []
                current_entry["authors"].append(line[5:].strip())
            elif line.startswith("PY  -"):
                current_entry["year"] = line[5:].strip()[:4]
            elif line.startswith("T2  -"):
                current_entry["journal"] = line[5:].strip()

        if current_entry and current_entry.get("doi"):
            papers.append(current_entry)

    return papers


async def download_papers(
    papers: List[Dict[str, str]],
    output_dir: str,
    sources: List[str],
    proxy: Optional[str],
    interactive: bool,
    wait_time: int,
    max_workers: int = 3,
) -> Dict:
    """æ‰¹é‡ä¸‹è½½è®ºæ–‡"""

    downloader = MultiChannelBrowserDownloader(
        proxy=proxy, headless=not interactive, download_dir=output_dir
    )

    results = {"total": len(papers), "success": 0, "failed": 0, "items": []}

    try:
        for i, paper in enumerate(papers, 1):
            doi = paper.get("doi", "")
            title = paper.get("title", "Unknown")

            logger.info(f"\n{'=' * 60}")
            logger.info(f"[{i}/{len(papers)}] å¤„ç†: {doi}")
            logger.info(f"  æ ‡é¢˜: {title[:60]}...")
            logger.info(f"{'=' * 60}")

            result = await downloader.download(
                doi, sources=sources, interactive=interactive, wait_time=wait_time
            )

            item = {
                "index": i,
                "doi": doi,
                "title": title,
                "status": "success" if result["success"] else "failed",
                "file": result.get("file"),
                "source": result.get("source"),
                "attempts": result.get("attempts", []),
            }

            results["items"].append(item)

            if result["success"]:
                results["success"] += 1
                logger.info(f"âœ… [{i}/{len(papers)}] ä¸‹è½½æˆåŠŸ: {result['file']}")
            else:
                results["failed"] += 1
                logger.warning(f"âŒ [{i}/{len(papers)}] ä¸‹è½½å¤±è´¥")

            # é¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(2)

    finally:
        await downloader.close()

    return results


def generate_report(results: Dict, output_dir: str, start_time: str, end_time: str):
    """ç”Ÿæˆä¸‹è½½æŠ¥å‘Š"""
    report = HTMLReportGenerator(output_dir, 3, 0)

    for item in results["items"]:
        attempts = [
            {
                "source": a.get("source", "unknown"),
                "retry": 1,
                "status": "success" if a.get("success") else "failed",
            }
            for a in item.get("attempts", [])
        ]

        report.add_item(
            index=item["index"],
            doi=item["doi"],
            status=item["status"],
            attempts=attempts,
            final_source=item.get("source"),
            file=item.get("file"),
            size=0,
        )

    report.update_summary(
        total=results["total"], success=results["success"], failed=results["failed"]
    )

    html_file = report.generate()

    # ç”Ÿæˆæ–‡æœ¬æ€»ç»“
    summary_file = os.path.join(output_dir, "download_summary.txt")
    with open(summary_file, "w", encoding="utf-8") as f:
        f.write(f"RIS æ–‡ä»¶æµè§ˆå™¨æ‰¹é‡ä¸‹è½½æ€»ç»“\n")
        f.write(f"æ—¶é—´: {end_time}\n")
        f.write(f"æ€»è®¡: {results['total']} ç¯‡\n")
        f.write(f"æˆåŠŸ: {results['success']} ç¯‡\n")
        f.write(f"å¤±è´¥: {results['failed']} ç¯‡\n")
        success_rate = (
            results["success"] / results["total"] * 100 if results["total"] > 0 else 0
        )
        f.write(f"æˆåŠŸç‡: {success_rate:.1f}%\n\n")

        f.write("æˆåŠŸåˆ—è¡¨:\n")
        for item in results["items"]:
            if item["status"] == "success":
                f.write(f"  {item['doi']}\n")
                f.write(f"    æ¥æº: {item.get('source', 'unknown')}\n")
                f.write(f"    æ–‡ä»¶: {item.get('file', 'N/A')}\n\n")

        f.write("\nå¤±è´¥åˆ—è¡¨:\n")
        for item in results["items"]:
            if item["status"] == "failed":
                f.write(f"  {item['doi']}\n")

    return html_file, summary_file


async def main():
    parser = argparse.ArgumentParser(description="æµè§ˆå™¨ç‰ˆ RIS æ–‡ä»¶æ‰¹é‡ä¸‹è½½å™¨")
    parser.add_argument("ris_file", help="RIS æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--output", "-o", default="./browser_downloads", help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--sources",
        "-s",
        nargs="+",
        default=["unpaywall", "semantic_scholar", "scihub"],
        help="ä¸‹è½½æº",
    )
    parser.add_argument("--proxy", help="ä»£ç†æœåŠ¡å™¨")
    parser.add_argument(
        "--interactive", "-i", action="store_true", help="äº¤äº’æ¨¡å¼ï¼ˆæ˜¾ç¤ºæµè§ˆå™¨ï¼‰"
    )
    parser.add_argument("--wait", type=int, default=30, help="äº¤äº’æ¨¡å¼ç­‰å¾…æ—¶é—´")

    args = parser.parse_args()

    if not os.path.exists(args.ris_file):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.ris_file}")
        sys.exit(1)

    proxy = args.proxy or os.environ.get("HTTP_PROXY") or "http://127.0.0.1:7897"

    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     æµè§ˆå™¨ç‰ˆ RIS æ‰¹é‡ä¸‹è½½å™¨ v1.0                   â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  RIS æ–‡ä»¶: {args.ris_file:<39}â•‘
â•‘  è¾“å‡ºç›®å½•: {args.output:<39}â•‘
â•‘  ä¸‹è½½æº: {str(args.sources):<41}â•‘
â•‘  ä»£ç†: {proxy:<43}â•‘
â•‘  äº¤äº’æ¨¡å¼: {str(args.interactive):<39}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    start_time = time.strftime("%Y-%m-%d %H:%M:%S")

    # è§£æ RIS æ–‡ä»¶
    logger.info(f"ğŸ“– è§£æ RIS æ–‡ä»¶: {args.ris_file}")
    papers = parse_ris_file(args.ris_file)
    logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(papers)} ç¯‡æ–‡çŒ®")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output, exist_ok=True)

    # æ‰¹é‡ä¸‹è½½
    results = await download_papers(
        papers, args.output, args.sources, proxy, args.interactive, args.wait
    )

    end_time = time.strftime("%Y-%m-%d %H:%M:%S")

    # ç”ŸæˆæŠ¥å‘Š
    html_file, summary_file = generate_report(
        results, args.output, start_time, end_time
    )

    # æ‰“å°æ€»ç»“
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  ğŸ“Š ä¸‹è½½æ€»ç»“                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  âœ… æˆåŠŸ: {results["success"]}/{results["total"]} ç¯‡                              â•‘
â•‘  âŒ å¤±è´¥: {results["failed"]}/{results["total"]} ç¯‡                              â•‘
â•‘  ğŸ“ˆ æˆåŠŸç‡: {results["success"] / results["total"] * 100:.1f}%                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    if results["success"] > 0:
        print("âœ… æˆåŠŸåˆ—è¡¨:")
        for item in results["items"]:
            if item["status"] == "success":
                print(f"   âœ“ {item['doi']}")
                print(f"     æ¥æº: {item.get('source', 'unknown')}")

    if results["failed"] > 0:
        print("\nâŒ å¤±è´¥åˆ—è¡¨:")
        for item in results["items"]:
            if item["status"] == "failed":
                print(f"   âœ— {item['doi']}")

    print(f"\nğŸ“ è¯¦ç»†æ—¥å¿—: {summary_file}")
    print(f"ğŸŒ HTML æŠ¥å‘Š: {html_file}")


if __name__ == "__main__":
    asyncio.run(main())
