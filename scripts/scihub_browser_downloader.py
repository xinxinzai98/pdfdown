#!/usr/bin/env python3
"""
Sci-Hub æµè§ˆå™¨è‡ªåŠ¨åŒ–ä¸‹è½½å™¨
ä½¿ç”¨ Selenium ç»•è¿‡åçˆ¬è™«ä¿æŠ¤
"""

import os
import sys
import time
import re
from urllib.parse import urljoin

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
except ImportError as e:
    print("é”™è¯¯: ç¼ºå°‘ä¾èµ–")
    print(f"è¯·è¿è¡Œ: pip3 install selenium webdriver-manager")
    print(f"ç¼ºå¤±çš„æ¨¡å—: {e}")
    sys.exit(1)


class SciHubBrowserDownloader:
    """ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–ä¸‹è½½ Sci-Hub æ–‡çŒ®"""

    def __init__(self, headless=True):
        """åˆå§‹åŒ–æµè§ˆå™¨ä¸‹è½½å™¨

        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        """
        self.headless = headless
        self.output_dir = "ris_downloads"

        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def _create_driver(self):
        """åˆ›å»º WebDriverï¼ˆè‡ªåŠ¨ç®¡ç†é©±åŠ¨ï¼Œæ”¯æŒä»£ç†ï¼‰"""
        options = Options()

        if self.headless:
            options.add_argument("--headless")

        # æ·»åŠ å…¶ä»–ä¼˜åŒ–é€‰é¡¹
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--disable-blink-features=AutomationControlled")

        # è®¾ç½® User-Agent
        options.add_argument(
            "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )

        # æ·»åŠ ä»£ç†æ”¯æŒ
        # æ³¨æ„: Chrome ä¸ç›´æ¥æ”¯æŒé€šè¿‡å‘½ä»¤è¡Œè®¾ç½®ä»£ç†ï¼Œéœ€è¦é€šè¿‡ Selenium çš„ä»£ç†é…ç½®
        # ä½†æˆ‘ä»¬å¯ä»¥åœ¨ä¸‹è½½ PDF æ—¶ä½¿ç”¨ä»£ç†

        # ä½¿ç”¨ webdriver-manager è‡ªåŠ¨ç®¡ç†é©±åŠ¨
        driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)
        driver.set_page_load_timeout(60)
        driver.set_script_timeout(30)

        return driver

    def download_from_scihub(self, doi):
        """ä» Sci-Hub ä¸‹è½½æ–‡çŒ®

        Args:
            doi: DOI

        Returns:
            dict: {"success": bool, "file": str, "size": int, "error": str}
        """
        scihub_domains = [
            "https://sci-hub.ru",
            "https://sci-hub.wf",
            "https://sci-hub.mksa.top",
            "https://sci-hub.st",
            "https://sci-hub.do",
        ]

        driver = None

        try:
            driver = self._create_driver()

            for domain in scihub_domains:
                try:
                    print(f"\nå°è¯•åŸŸå: {domain}")
                    url = f"{domain}/{doi.replace('/', '%2F')}"

                    print(f"  è®¿é—®: {url}")

                    # ç»•è¿‡ä»£ç†è®¿é—® Sci-Hub é¡µé¢ï¼ˆè®©æµè§ˆå™¨ç›´æ¥è®¿é—®ï¼‰
                    driver.get(url)

                    # ç­‰å¾…é¡µé¢åŠ è½½
                    print(f"  ç­‰å¾…é¡µé¢åŠ è½½...")
                    time.sleep(3)  # ç»™ JavaScript æ‰§è¡Œæ—¶é—´

                    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
                    WebDriverWait(driver, 30).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )

                    # è·å–é¡µé¢ HTML
                    html = driver.page_source
                    current_url = driver.current_url

                    print(f"  å½“å‰ URL: {current_url}")
                    print(f"  é¡µé¢é•¿åº¦: {len(html)} å­—ç¬¦")

                    # æ£€æŸ¥æ˜¯å¦æ˜¯ DDoS-Guard æŒ‘æˆ˜é¡µé¢
                    if "DDoS-Guard" in html:
                        print(f"  âŒ è¢« DDoS-Guard ä¿æŠ¤")
                        continue

                    # æ£€æŸ¥æ˜¯å¦ç›´æ¥æ˜¯ PDF
                    content_type = self._get_content_type_from_driver(driver)
                    print(f"  Content-Type: {content_type}")

                    if "pdf" in content_type.lower() or current_url.lower().endswith(
                        ".pdf"
                    ):
                        print(f"  âœ… æ£€æµ‹åˆ° PDF ç›´æ¥å“åº”")

                        # ç›´æ¥ä¸‹è½½ PDF
                        filename = f"SciHub_Browser_{doi.replace('/', '_').replace('.', '_')}.pdf"
                        filepath = os.path.join(self.output_dir, filename)

                        # ä½¿ç”¨ requests ä¸‹è½½ PDF
                        import requests

                        proxies = {
                            "http": "http://127.0.0.1:7897",
                            "https": "http://127.0.0.1:7897",
                        }

                        response = requests.get(
                            current_url, proxies=proxies, timeout=30, stream=True
                        )

                        if response.status_code == 200:
                            with open(filepath, "wb") as f:
                                for chunk in response.iter_content(chunk_size=8192):
                                    f.write(chunk)

                            file_size = os.path.getsize(filepath)

                            print(f"  âœ… ä¸‹è½½æˆåŠŸ!")
                            print(f"     æ–‡ä»¶: {filename}")
                            print(f"     å¤§å°: {file_size:,} bytes")

                            driver.quit()
                            return {
                                "success": True,
                                "file": filepath,
                                "size": file_size,
                            }

                    # æŸ¥æ‰¾ PDF é“¾æ¥
                    pdf_links = self._extract_pdf_links(html, current_url)

                    if pdf_links:
                        print(f"  âœ… æ‰¾åˆ° {len(pdf_links)} ä¸ª PDF é“¾æ¥")

                        for i, pdf_url in enumerate(pdf_links[:3], 1):
                            print(f"  [{i}] {pdf_url}")

                            # å°è¯•ä¸‹è½½
                            result = self._download_pdf(pdf_url, doi, f"SciHub_Browser")

                            if result["success"]:
                                driver.quit()
                                return result

                            time.sleep(1)

                    # æŸ¥æ‰¾åµŒå…¥çš„ PDF
                    embed_pdfs = self._extract_embed_pdfs(html, domain)

                    if embed_pdfs:
                        print(f"  âœ… æ‰¾åˆ° {len(embed_pdfs)} ä¸ªåµŒå…¥ PDF")

                        for i, pdf_url in enumerate(embed_pdfs[:2], 1):
                            print(f"  [{i}] {pdf_url}")

                            result = self._download_pdf(pdf_url, doi, f"SciHub_Browser")

                            if result["success"]:
                                driver.quit()
                                return result

                            time.sleep(1)

                    print(f"  âŒ æœªæ‰¾åˆ°å¯ä¸‹è½½çš„ PDF")

                except Exception as e:
                    print(f"  âŒ åŸŸå {domain} å¤±è´¥: {str(e)[:100]}")
                    continue

            driver.quit()
            return {"success": False, "error": "æ‰€æœ‰åŸŸåå‡å¤±è´¥"}

        except Exception as e:
            if driver:
                driver.quit()
            return {"success": False, "error": str(e)}

    def _get_content_type_from_driver(self, driver):
        """ä» WebDriver è·å– Content-Type"""
        try:
            content_type = driver.execute_script(
                "return document.contentType || 'unknown';"
            )
            return content_type
        except:
            return "unknown"

    def _extract_pdf_links(self, html, base_url):
        """ä» HTML ä¸­æå– PDF é“¾æ¥"""
        pdf_links = []

        # æ–¹æ³•1: href å±æ€§
        pattern = re.compile(r'href=["\']([^"\']*\.pdf[^"\']*)["\']', re.IGNORECASE)
        matches = pattern.findall(html)

        for match in matches:
            if match and match != "#" and "sci-hub" not in match.lower():
                if not match.startswith("http"):
                    match = urljoin(base_url, match)
                pdf_links.append(match)

        # æ–¹æ³•2: onclick äº‹ä»¶
        pattern2 = re.compile(
            r'onclick=["\'][^"\']*location\s*=\s*[\'"]([^"\']+\.pdf[^"\']*)["\']',
            re.IGNORECASE,
        )
        matches2 = pattern2.findall(html)

        for match in matches2:
            if match and "sci-hub" not in match.lower():
                if not match.startswith("http"):
                    match = urljoin(base_url, match)
                pdf_links.append(match)

        return list(set(pdf_links))  # å»é‡

    def _extract_embed_pdfs(self, html, base_url):
        """ä» HTML ä¸­æå–åµŒå…¥çš„ PDF"""
        embed_pdfs = []

        # æŸ¥æ‰¾ embed æ ‡ç­¾
        pattern = re.compile(r'<embed[^>]*src=["\']([^"\']+)["\']', re.IGNORECASE)
        matches = pattern.findall(html)

        for match in matches:
            if match and match.endswith(".pdf"):
                if match.startswith("//"):
                    match = "https:" + match
                elif not match.startswith("http"):
                    match = urljoin(base_url, match)
                embed_pdfs.append(match)

        return embed_pdfs

    def _download_pdf(self, url, doi, source):
        """ä¸‹è½½ PDF"""
        import requests

        proxies = {"http": "http://127.0.0.1:7897", "https": "http://127.0.0.1:7897"}

        try:
            response = requests.get(url, proxies=proxies, timeout=30, stream=True)

            if response.status_code == 200:
                content_type = response.headers.get("Content-Type", "").lower()

                if "pdf" in content_type or url.lower().endswith(".pdf"):
                    safe_doi = doi.replace("/", "_").replace(".", "_")
                    filename = f"{source}_{safe_doi}.pdf"
                    filepath = os.path.join(self.output_dir, filename)

                    with open(filepath, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)

                    file_size = os.path.getsize(filepath)

                    print(f"    âœ… ä¸‹è½½æˆåŠŸ!")
                    print(f"       æ–‡ä»¶: {filename}")
                    print(f"       å¤§å°: {file_size:,} bytes")

                    return {"success": True, "file": filepath, "size": file_size}

            return {"success": False}

        except Exception as e:
            return {"success": False, "error": str(e)}


def main():
    """ä¸»å‡½æ•°"""
    import sys

    doi = "10.3390/pr8020248"

    if len(sys.argv) > 1:
        doi = sys.argv[1]

    headless = True
    if len(sys.argv) > 2 and sys.argv[2] == "--show":
        headless = False

    print("=" * 70)
    print("ğŸ§ª Sci-Hub æµè§ˆå™¨è‡ªåŠ¨åŒ–ä¸‹è½½å™¨")
    print("=" * 70)
    print(f"\nDOI: {doi}")
    print(f"æ— å¤´æ¨¡å¼: {'æ˜¯' if headless else 'å¦'}")

    downloader = SciHubBrowserDownloader(headless=headless)

    print("\nå¼€å§‹ä¸‹è½½...")
    print("=" * 70)

    result = downloader.download_from_scihub(doi)

    print("\n" + "=" * 70)
    if result["success"]:
        print("âœ… ä¸‹è½½æˆåŠŸ!")
        print(f"æ–‡ä»¶: {result['file']}")
        print(f"å¤§å°: {result['size']:,} bytes")
    else:
        print("âŒ ä¸‹è½½å¤±è´¥!")
        if "error" in result:
            print(f"é”™è¯¯: {result['error']}")

    print("=" * 70)


if __name__ == "__main__":
    main()
