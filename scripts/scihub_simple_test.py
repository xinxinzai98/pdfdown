#!/usr/bin/env python3
"""
Sci-Hub ç®€åŒ–æµ‹è¯•ç‰ˆ
åŸºäº GitHub ä¸Šçš„å®ç°æ–¹å¼
"""

import os
import requests
from bs4 import BeautifulSoup


def test_scihub_improved(doi, output_dir="ris_downloads"):
    """æµ‹è¯•æ”¹è¿›ç‰ˆ Sci-Hub ä¸‹è½½"""

    # æ–°åŸŸååˆ—è¡¨ï¼ˆæ¥è‡ª GitHub å®ç°ï¼‰
    scihub_domains = [
        "https://www.sci-hub.ren",  # æ–°åŸŸå âœ…
        "https://sci-hub.hk",  # æ–°åŸŸå âœ…
        "https://sci-hub.la",  # æ–°åŸŸå âœ…
        "https://sci-hub.cat",
        "https://sci-hub.ee",
        "https://sci-hub.se",
        "https://sci-hub.st",
        "https://sci-hub.ru",
        "sci-hub.wf",
        "sci-hub.yt",
        "sci-hub.do",
        "https://sci-hub.mksa.top",
        "https://www.tes1e.com",
    ]

    # Windows User-Agentï¼ˆå‚è€ƒ GitHubï¼‰
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    }

    # ä»£ç†é…ç½®
    proxies = {
        "http": "http://127.0.0.1:7897",
        "https": "http://127.0.0.1:7897",
    }

    print("=" * 70)
    print("ğŸ§ª Sci-Hub ç®€åŒ–æµ‹è¯•ç‰ˆ")
    print("=" * 70)
    print(f"DOI: {doi}")
    print()

    for domain in scihub_domains:
        try:
            url = f"{domain}/{doi.replace('/', '%2F')}"
            print(f"å°è¯•åŸŸå: {domain}")

            response = requests.get(
                url, headers=headers, proxies=proxies, timeout=30, allow_redirects=True
            )
            print(f"  çŠ¶æ€ç : {response.status_code}")

            if response.status_code != 200:
                print(f"  âŒ çŠ¶æ€ç é”™è¯¯")
                continue

            # æ£€æŸ¥ä¿æŠ¤
            if "DDoS-Guard" in response.text:
                print(f"  âŒ è¢« DDoS-Guard ä¿æŠ¤")
                continue

            # ä½¿ç”¨ BeautifulSoup è§£æ
            soup = BeautifulSoup(response.text, "html.parser")

            # æŸ¥æ‰¾ embed æ ‡ç­¾
            embed = soup.find("embed")
            if embed:
                embed_src = embed.get("src", "")
                if embed_src:
                    print(f"  âœ… æ‰¾åˆ° embed æ ‡ç­¾")
                    print(f"     src: {embed_src[:80]}...")

                    # å°è¯•ä¸‹è½½
                    result = download_pdf(embed_src, doi, output_dir, headers, proxies)
                    if result["success"]:
                        print(f"\\nâœ… ä¸‹è½½æˆåŠŸ!")
                        return result
                    else:
                        print(f"  âŒ ä¸‹è½½å¤±è´¥")

            # æŸ¥æ‰¾ iframe æ ‡ç­¾
            iframe = soup.find("iframe")
            if iframe:
                iframe_src = iframe.get("src", "")
                if iframe_src:
                    print(f"  âœ… æ‰¾åˆ° iframe æ ‡ç­¾")
                    print(f"     src: {iframe_src[:80]}...")

                    result = download_pdf(iframe_src, doi, output_dir, headers, proxies)
                    if result["success"]:
                        print(f"\\nâœ… ä¸‹è½½æˆåŠŸ!")
                        return result
                    else:
                        print(f"  âŒ ä¸‹è½½å¤±è´¥")

            # æŸ¥æ‰¾ PDF é“¾æ¥
            pdf_links = soup.find_all("a", href=True)
            pdf_count = 0
            for link in pdf_links:
                href = link.get("href", "")
                if href and ".pdf" in href.lower() and "sci-hub" not in href.lower():
                    pdf_count += 1
                    if pdf_count <= 3:
                        print(f"  [{pdf_count}] {href[:80]}...")

                    result = download_pdf(href, doi, output_dir, headers, proxies)
                    if result["success"]:
                        print(f"\\nâœ… ä¸‹è½½æˆåŠŸ!")
                        return result
                    else:
                        print(f"  âŒ ä¸‹è½½å¤±è´¥")

            print(f"  âŒ æœªæ‰¾åˆ°å¯ä¸‹è½½çš„ PDF")

        except Exception as e:
            print(f"  âŒ é”™è¯¯: {str(e)[:80]}")
            continue

    return {"success": False, "error": "æ‰€æœ‰åŸŸåå‡å¤±è´¥"}


def download_pdf(pdf_url, doi, output_dir, headers, proxies):
    """ä¸‹è½½ PDF"""
    try:
        response = requests.get(
            pdf_url, headers=headers, proxies=proxies, timeout=30, stream=True
        )

        if response.status_code == 200:
            content_type = response.headers.get("Content-Type", "").lower()

            if "pdf" in content_type or pdf_url.lower().endswith(".pdf"):
                safe_doi = doi.replace("/", "_").replace(".", "_")
                filename = f"SciHub_Improved_{safe_doi}.pdf"
                filepath = os.path.join(output_dir, filename)

                with open(filepath, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                file_size = os.path.getsize(filepath)

                print(f"     âœ… æ–‡ä»¶: {filename}")
                print(f"     å¤§å°: {file_size:,} bytes")

                return {"success": True, "file": filepath, "size": file_size}

            return {"success": False}

    except Exception as e:
        return {"success": False, "error": str(e)}


def main():
    import sys

    doi = "10.3390/pr8020248"

    if len(sys.argv) > 1:
        doi = sys.argv[1]

    print("=" * 70)
    print("ğŸ§ª Sci-Hub ç®€åŒ–æµ‹è¯•ç‰ˆ")
    print("=" * 70)
    print(f"DOI: {doi}")
    print()

    downloader_result = test_scihub_improved(doi)

    print("\\n" + "=" * 70)
    if downloader_result["success"]:
        print("âœ… ä¸‹è½½æˆåŠŸ!")
        print(f"æ–‡ä»¶: {downloader_result['file']}")
        print(f"å¤§å°: {downloader_result['size']:,} bytes")
    else:
        print("âŒ ä¸‹è½½å¤±è´¥")
        if "error" in downloader_result:
            print(f"é”™è¯¯: {downloader_result['error']}")
    print("=" * 70)


if __name__ == "__main__":
    main()
