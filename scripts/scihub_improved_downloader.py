#!/usr/bin/env python3
"""
Sci-Hub æ”¹è¿›ç‰ˆä¸‹è½½æµ‹è¯•å™¨
åŸºäº GitHub ä¸Šçš„å®ç°æ–¹å¼
"""

import os
import re
import time
from urllib.parse import urljoin
import requests

try:
    from bs4 import BeautifulSoup
except ImportError:
    print("é”™è¯¯: æœªå®‰è£… BeautifulSoup4")
    print("è¯·è¿è¡Œ: pip3 install beautifulsoup4")
    sys.exit(1)


class SciHubImprovedDownloader:
    """æ”¹è¿›ç‰ˆ Sci-Hub ä¸‹è½½å™¨"""

    def __init__(self, output_dir="ris_downloads"):
        self.output_dir = output_dir
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        # ä½¿ç”¨ GitHub å®ç°ä¸­çš„æ–°åŸŸå
        self.scihub_domains = [
            "https://www.sci-hub.ren",  # æ–°åŸŸå âœ… å¯ç”¨
            "https://sci-hub.hk",
            "https://sci-hub.la",  # æ–°åŸŸå âœ… å¯ç”¨
            "https://sci-hub.cat",
            "https://sci-hub.ee",
            "https://sci-hub.se",
            "https://sci-hub.st",
            "https://sci-hub.ru",
            "sci-hub.wf",
            "sci-hub.yt",
            "sci-hub.do",
            "https://sci-hub.mksa.top",
            "https://www.tes1e.com",  # æ–°åŸŸå
        ]

        # ä½¿ç”¨ Windows User-Agentï¼ˆå‚è€ƒ GitHub å®ç°ï¼‰
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
        }

        self.session = requests.Session()
        self.session.headers.update(self.headers)
        # ä½¿ç”¨æµ·å¤–ä»£ç†
        self.proxies = {
            "http": "http://127.0.0.1:7897",
            "https": "http://127.0.0.1:7897",
        }

    def download(self, doi):
        """ä» Sci-Hub ä¸‹è½½æ–‡çŒ®"""

        print(f"\\nå°è¯•ä¸‹è½½: {doi}")

        for domain in self.scihub_domains:
            try:
                url = f"{domain}/{doi.replace('/', '%2F')}"
                print(f"\\nåŸŸå: {domain}")
                print(f"URL: {url}")

                response = self.session.get(
                    url, proxies=self.proxies, timeout=30, allow_redirects=True
                )
                print(f"  çŠ¶æ€ç : {response.status_code}")

                if response.status_code != 200:
                    print(f"  âŒ çŠ¶æ€ç é”™è¯¯")
                    continue

                # ä½¿ç”¨ BeautifulSoup è§£æ
                soup = BeautifulSoup(response.text, "html.parser")

                # æ£€æŸ¥æ˜¯å¦è¢« DDoS-Guard ä¿æŠ¤
                if "DDoS-Guard" in response.text:
                    print(f"  âŒ è¢« DDoS-Guard ä¿æŠ¤")
                    continue

                # æ–¹æ³•1: æŸ¥æ‰¾ embed æ ‡ç­¾ï¼ˆGitHub æ–¹æ³•ï¼‰
                embed = soup.find("embed")
                if embed:
                    embed_src = embed.get("src", "")
                    if embed_src:
                        embed_src_str = str(embed_src)
                        print(f"  âœ“ æ‰¾åˆ° embed æ ‡ç­¾")
                        print(f"    src: {embed_src_str[:80]}...")

                        # ç¡®ä¿ URL æ˜¯å®Œæ•´çš„
                        if embed_src_str.startswith("//"):
                            embed_src_str = "https:" + embed_src_str
                        elif not embed_src_str.startswith("http"):
                            embed_src_str = urljoin(response.url, embed_src_str)

                        # å°è¯•ä¸‹è½½
                        result = self._download_pdf(embed_src_str, doi)
                        if result["success"]:
                            return result
                        else:
                            print(f"    âŒ ä¸‹è½½å¤±è´¥")

                # æ–¹æ³•2: æŸ¥æ‰¾ iframe æ ‡ç­¾
                iframe = soup.find("iframe")
                if iframe:
                    iframe_src = iframe.get("src", "")
                    if iframe_src:
                        iframe_src_str = str(iframe_src)
                        print(f"  âœ“ æ‰¾åˆ° iframe æ ‡ç­¾")
                        print(f"    src: {iframe_src_str[:80]}...")

                        # ç¡®ä¿ URL æ˜¯å®Œæ•´çš„
                        if iframe_src_str.startswith("//"):
                            iframe_src_str = "https:" + iframe_src_str
                        elif not iframe_src_str.startswith("http"):
                            iframe_src_str = urljoin(response.url, iframe_src_str)

                        result = self._download_pdf(iframe_src_str, doi)
                        if result["success"]:
                            return result
                        else:
                            print(f"    âŒ ä¸‹è½½å¤±è´¥")

                # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰ PDF é“¾æ¥
                pdf_links = soup.find_all("a", href=True)
                pdf_links = [
                    l
                    for l in pdf_links
                    if l.get("href") and ".pdf" in str(l.get("href", ""))
                ]

                if pdf_links:
                    print(f"  âœ“ æ‰¾åˆ° {len(pdf_links)} ä¸ª PDF é“¾æ¥")

                    for i, link in enumerate(pdf_links[:3], 1):
                        href = link.get("href", "")
                        if href and "sci-hub" not in href.lower():
                            href_str = str(href)
                            if not href_str.startswith("http"):
                                href_str = urljoin(response.url, href_str)

                            result = self._download_pdf(href_str, doi)
                            if result["success"]:
                                return result
                            else:
                                print(f"    [{i}] ä¸‹è½½å¤±è´¥")

                print(f"  âŒ æœªæ‰¾åˆ°å¯ä¸‹è½½çš„ PDF")

            except Exception as e:
                print(f"  âŒ é”™è¯¯: {str(e)[:80]}")
                continue

        return {"success": False, "error": "æ‰€æœ‰åŸŸåå‡å¤±è´¥"}

    def _download_pdf(self, pdf_url, doi):
        """ä¸‹è½½ PDF"""
        try:
            response = self.session.get(
                pdf_url, proxies=self.proxies, timeout=30, stream=True
            )

            if response.status_code == 200:
                content_type = response.headers.get("Content-Type", "").lower()

                if "pdf" in content_type or pdf_url.lower().endswith(".pdf"):
                    safe_doi = doi.replace("/", "_").replace(".", "_")
                    filename = f"SciHub_{safe_doi}.pdf"
                    filepath = os.path.join(self.output_dir, filename)

                    with open(filepath, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)

                    file_size = os.path.getsize(filepath)

                    print(f"    âœ… ä¸‹è½½æˆåŠŸ!")
                    print(f"    æ–‡ä»¶: {filename}")
                    print(f"    å¤§å°: {file_size:,} bytes")

                    # éªŒè¯ PDF
                    with open(filepath, "rb") as f:
                        header = f.read(4)
                        tail = f.read()[-100:]

                    if header == b"%PDF" and b"%EOF" in tail:
                        print(f"    âœ… PDF éªŒè¯é€šè¿‡")
                    else:
                        print(f"    âš ï¸ PDF å¯èƒ½æŸå")

                    return {"success": True, "file": filepath, "size": file_size}

            return {"success": False}

        except Exception as e:
            return {"success": False, "error": str(e)}


def main():
    """ä¸»å‡½æ•°"""
    import sys

    doi = "10.3390/pr8020248"

    if len(sys.argv) > 1:
        doi = sys.argv[1]

    print("=" * 70)
    print("ğŸ§ª Sci-Hub æ”¹è¿›ç‰ˆä¸‹è½½å™¨")
    print("=" * 70)

    downloader = SciHubImprovedDownloader()

    start = time.time()
    result = downloader.download(doi)
    elapsed = time.time() - start

    print(f"\\næ€»è€—æ—¶: {elapsed:.1f} ç§’")

    print("\\n" + "=" * 70)
    if result["success"]:
        print("âœ… ä¸‹è½½æˆåŠŸ!")
        print(f"æ–‡ä»¶: {result['file']}")
        print(f"å¤§å°: {result['size']:,} bytes")
    else:
        print("âŒ ä¸‹è½½å¤±è´¥")
        if "error" in result:
            print(f"é”™è¯¯: {result['error']}")
    print("=" * 70)


if __name__ == "__main__":
    main()
