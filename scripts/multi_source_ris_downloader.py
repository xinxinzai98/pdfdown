#!/usr/bin/env python3
"""
RIS æ–‡ä»¶å¤šæ¸ é“æ‰¹é‡ä¸‹è½½å™¨

æ”¯æŒå¤šç§ä¸‹è½½æºï¼Œæé«˜æˆåŠŸç‡
"""

import re
import os
import sys
import time
import requests
from urllib.parse import quote, urljoin


class MultiSourceDownloader:
    """å¤šæ¥æºä¸‹è½½å™¨"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            }
        )

        self.output_dir = "ris_downloads"
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        self.results = {"success": [], "failed": []}

    def download_doi(self, doi, source_name="auto"):
        """å°è¯•ä»å¤šä¸ªæ¥æºä¸‹è½½å•ä¸ª DOI

        Args:
            doi: DOI
            source_name: æ¥æºåç§°
        """
        sources = [
            ("Unpaywall API", self._try_unpaywall),
            ("Sci-Hub âš ï¸", self._try_scihub),
            ("Semantic Scholar", self._try_semantic_scholar),
            ("arXiv", self._try_arxiv),
            ("CORE", self._try_core),
            ("Open Access Button", self._try_openaccess),
        ]

        for source_name, download_func in sources:
            try:
                print(f"  [{source_name}] ...", end=" ")
                result = download_func(doi)

                if result and result.get("success"):
                    print(f"âœ… æˆåŠŸ")
                    self.results["success"].append(
                        {"doi": doi, "source": source_name, "file": result.get("file")}
                    )
                    return True
                else:
                    print(f"âŒ å¤±è´¥")

                # çŸ­æš‚å»¶è¿Ÿ
                time.sleep(0.5)

            except Exception as e:
                print(f"âŒ é”™è¯¯: {str(e)[:50]}")

        return False

    def _try_unpaywall(self, doi):
        """å°è¯• Unpaywall API"""
        try:
            url = f"https://api.unpaywall.org/v2/{doi}?email=your@email.com"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                data = response.json()
                if data.get("is_oa"):
                    pdf_url = data.get("best_oa_location", {}).get("url")
                    if pdf_url:
                        return self._download_and_save(pdf_url, doi, "Unpaywall")

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_semantic_scholar(self, doi):
        """å°è¯• Semantic Scholar"""
        try:
            url = f"https://api.semanticscholar.org/v1/paper/DOI:{doi}"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                data = response.json()

                # æ£€æŸ¥å¼€æ”¾è·å–
                oa_pdf = data.get("openAccessPdf")
                if oa_pdf:
                    pdf_url = oa_pdf.get("url")
                    if pdf_url:
                        return self._download_and_save(pdf_url, doi, "Semantic_Scholar")

                # æ£€æŸ¥æ¥æº
                sources = data.get("sources", [])
                for source in sources:
                    url = source.get("url")
                    if url and "pdf" in url.lower():
                        return self._download_and_save(url, doi, "Semantic_Scholar")

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_arxiv(self, doi):
        """å°è¯• arXiv"""
        # æ£€æŸ¥æ˜¯å¦æ˜¯ arXiv è®ºæ–‡
        if "arxiv" not in doi.lower():
            return {"success": False}

        # æå– arXiv ID
        arxiv_pattern = re.compile(r"(?:10\.\d+/)?arxiv\.?/?(\d+\.\d+)", re.IGNORECASE)
        match = arxiv_pattern.search(doi)

        if match:
            arxiv_id = match.group(1)
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            return self._download_and_save(pdf_url, doi, "arXiv")

        return {"success": False}

    def _try_core(self, doi):
        """å°è¯• CORE"""
        try:
            url = f"https://core.ac.uk/search?q={quote(doi)}"
            response = self.session.get(url, timeout=10)

            # æŸ¥æ‰¾ PDF é“¾æ¥
            pdf_pattern = re.compile(
                r'href=["\']([^"\']*\.pdf[^"\']*)["\']', re.IGNORECASE
            )
            pdf_links = pdf_pattern.findall(response.text)

            if pdf_links:
                for pdf_url in pdf_links[:3]:
                    if pdf_url.startswith("https://core.ac.uk/download"):
                        return self._download_and_save(pdf_url, doi, "CORE")

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_openaccess(self, doi):
        """å°è¯•å¼€æ”¾è·å–æŒ‰é’®"""
        try:
            url = f"https://api.openaccessbutton.org/v2/{doi}?email=your@email.com"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                data = response.json()
                if data.get("status") == "success" and data.get("file_type") == "pdf":
                    pdf_url = data.get("file_url")
                    if pdf_url:
                        return self._download_and_save(pdf_url, doi, "OpenAccess")

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_scihub(self, doi):
        """å°è¯• Sci-Hub ä¸‹è½½

        æ³¨æ„: Sci-Hub å­˜åœ¨æ³•å¾‹é£é™©ï¼Œè¯·è°¨æ…ä½¿ç”¨
        """
        scihub_domains = [
            "https://sci-hub.se",
            "https://sci-hub.st",
            "https://sci-hub.ru",
            "https://sci-hub.wf",
            "https://sci-hub.yt",
            "https://sci-hub.do",
        ]

        for domain in scihub_domains:
            try:
                # æ„å»º Sci-Hub URL
                url = f"{domain}/{doi.replace('/', '%2F')}"

                response = self.session.get(url, timeout=30, allow_redirects=True)

                if response.status_code == 200:
                    # æŸ¥æ‰¾ PDF é“¾æ¥
                    # Sci-Hub é€šå¸¸ä¼šåœ¨å“åº”ä¸­ç›´æ¥æ˜¾ç¤º PDFï¼Œæˆ–æä¾›ä¸‹è½½é“¾æ¥

                    # æ–¹æ³•1: æŸ¥æ‰¾ PDF é“¾æ¥
                    pdf_pattern = re.compile(
                        r'href=["\']([^"\']*\.pdf[^"\']*)["\']', re.IGNORECASE
                    )
                    pdf_links = pdf_pattern.findall(response.text)

                    for pdf_url in pdf_links:
                        if (
                            pdf_url
                            and pdf_url != "#"
                            and "sci-hub" not in pdf_url.lower()
                        ):
                            if not pdf_url.startswith("http"):
                                pdf_url = urljoin(response.url, pdf_url)

                            # å°è¯•ä¸‹è½½
                            result = self._download_and_save(pdf_url, doi, "SciHub")
                            if result.get("success"):
                                return result

                    # æ–¹æ³•2: æ£€æŸ¥å“åº”æ˜¯å¦ç›´æ¥æ˜¯ PDF
                    content_type = response.headers.get("Content-Type", "").lower()

                    if "pdf" in content_type or url.lower().endswith(".pdf"):
                        # ä¿å­˜ PDF
                        filename = (
                            f"SciHub_{doi.replace('/', '_').replace('.', '_')}.pdf"
                        )
                        filepath = os.path.join(self.output_dir, filename)

                        with open(filepath, "wb") as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)

                        file_size = os.path.getsize(filepath)

                        print(f"    ğŸ“ {filename} ({file_size:,} bytes)")

                        return {"success": True, "file": filepath, "size": file_size}

                    # æ–¹æ³•3: æŸ¥æ‰¾åµŒå…¥çš„ PDF
                    embed_pdf_pattern = re.compile(
                        r'<embed[^>]*src=["\']([^"\']+)["\']', re.IGNORECASE
                    )
                    embed_matches = embed_pdf_pattern.findall(response.text)

                    for embed_url in embed_matches:
                        if embed_url and embed_url.endswith(".pdf"):
                            if embed_url.startswith("//"):
                                embed_url = "https:" + embed_url
                            elif not embed_url.startswith("http"):
                                embed_url = urljoin(domain, embed_url)

                            result = self._download_and_save(embed_url, doi, "SciHub")
                            if result.get("success"):
                                return result

            except requests.exceptions.RequestException:
                continue
            except Exception as e:
                continue

        return {"success": False}

    def _download_and_save(self, url, doi, source):
        """ä¸‹è½½å¹¶ä¿å­˜ PDF"""
        try:
            response = self.session.get(url, timeout=30, stream=True)

            if response.status_code == 200:
                content_type = response.headers.get("Content-Type", "").lower()

                if "pdf" in content_type or url.lower().endswith(".pdf"):
                    # ç”Ÿæˆæ–‡ä»¶å
                    safe_doi = doi.replace("/", "_").replace(".", "_")
                    filename = f"{source}_{safe_doi}.pdf"
                    filepath = os.path.join(self.output_dir, filename)

                    # ä¿å­˜æ–‡ä»¶
                    with open(filepath, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)

                    file_size = os.path.getsize(filepath)

                    print(f"    ğŸ“ {filename} ({file_size:,} bytes)")

                    return {"success": True, "file": filepath, "size": file_size}

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def batch_download_from_ris(self, ris_file):
        """ä» RIS æ–‡ä»¶æ‰¹é‡ä¸‹è½½"""

        # æå–æ‰€æœ‰ DOI
        dois = []
        with open(ris_file, "r", encoding="utf-8") as f:
            content = f.read()

        doi_pattern = re.compile(r"^DO\s*-\s*(.+)$", re.MULTILINE)
        matches = doi_pattern.findall(content)

        for doi in matches:
            doi = doi.strip()
            if doi and doi not in dois:
                dois.append(doi)

        print("=" * 70)
        print("ğŸ“š RIS æ–‡ä»¶å¤šæ¸ é“æ‰¹é‡ä¸‹è½½å™¨")
        print("=" * 70)
        print(f"\nğŸ“„ RIS æ–‡ä»¶: {ris_file}")
        print(f"ğŸ“‹ æ‰¾åˆ° {len(dois)} ä¸ª DOI:")
        for i, doi in enumerate(dois, 1):
            print(f"  [{i}] {doi}")

        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½...")
        print(f"ğŸ“ ä¿å­˜ç›®å½•: {self.output_dir}")
        print("=" * 70)

        # æ‰¹é‡ä¸‹è½½
        for i, doi in enumerate(dois, 1):
            print(f"\n[{i}/{len(dois)}] {doi}")
            print("-" * 70)

            success = self.download_doi(doi)

            if success:
                print(f"âœ… {doi} ä¸‹è½½æˆåŠŸ")
            else:
                print(f"âŒ {doi} æ‰€æœ‰æ¥æºå‡å¤±è´¥")
                self.results["failed"].append(doi)

            # å»¶è¿Ÿ
            if i < len(dois):
                time.sleep(2)

        # æ‰“å°æ€»ç»“
        self.print_summary(dois)

    def print_summary(self, dois):
        """æ‰“å°ä¸‹è½½æ€»ç»“"""
        print("\n" + "=" * 70)
        print("ğŸ“Š ä¸‹è½½æ€»ç»“")
        print("=" * 70)

        print(f"\nâœ… æˆåŠŸ: {len(self.results['success'])} ç¯‡")
        for item in self.results["success"]:
            print(f"   âœ“ {item['doi']}")
            print(f"     æ¥æº: {item['source']}")
            print(f"     æ–‡ä»¶: {item['file']}")

        print(f"\nâŒ å¤±è´¥: {len(self.results['failed'])} ç¯‡")
        for doi in self.results["failed"]:
            print(f"   âœ— {doi}")

        success_rate = len(self.results["success"]) / len(dois) * 100
        print(f"\nğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")

        # ä¿å­˜æ—¥å¿—
        log_file = os.path.join(self.output_dir, "download_summary.txt")
        with open(log_file, "w", encoding="utf-8") as f:
            f.write(f"RIS æ–‡ä»¶æ‰¹é‡ä¸‹è½½æ€»ç»“\n")
            f.write(f"æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»è®¡: {len(dois)} ç¯‡\n")
            f.write(f"æˆåŠŸ: {len(self.results['success'])} ç¯‡\n")
            f.write(f"å¤±è´¥: {len(self.results['failed'])} ç¯‡\n")
            f.write(f"æˆåŠŸç‡: {success_rate:.1f}%\n\n")

            f.write("æˆåŠŸåˆ—è¡¨:\n")
            for item in self.results["success"]:
                f.write(f"  {item['doi']}\n")
                f.write(f"    æ¥æº: {item['source']}\n")
                f.write(f"    æ–‡ä»¶: {item['file']}\n\n")

            f.write("å¤±è´¥åˆ—è¡¨:\n")
            for doi in self.results["failed"]:
                f.write(f"  {doi}\n\n")

        print(f"\nğŸ“ è¯¦ç»†æ—¥å¿—: {log_file}")

        # æä¾›å»ºè®®
        print("\n" + "=" * 70)
        print("ğŸ’¡ æ‰‹åŠ¨ä¸‹è½½å»ºè®®")
        print("=" * 70)

        if self.results["failed"]:
            print("\nå¯¹äºæœªä¸‹è½½çš„æ–‡çŒ®ï¼Œå¯ä»¥å°è¯•ï¼š")
            print("  1. Google Scholar: https://scholar.google.com/")
            print("  2. Sci-Hub: https://sci-hub.se/ (è°¨æ…ä½¿ç”¨)")
            print("  3. ResearchGate: https://www.researchgate.net/")
            print("  4. å›¾ä¹¦é¦†èµ„æº")
            print("  5. è”ç³»ä½œè€…")


def main():
    """ä¸»å‡½æ•°"""
    # é»˜è®¤ RIS æ–‡ä»¶
    ris_file = "/Users/sanada/downloads/savedrecs.ris"

    # æ£€æŸ¥å‚æ•°
    if len(sys.argv) > 1:
        ris_file = sys.argv[1]

    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(ris_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {ris_file}")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python3 multi_source_ris_downloader.py [ris_file]")
        print("\nç¤ºä¾‹:")
        print("  python3 multi_source_ris_downloader.py savedrecs.ris")
        sys.exit(1)

    # åˆ›å»ºä¸‹è½½å™¨
    downloader = MultiSourceDownloader()

    # æ‰¹é‡ä¸‹è½½
    downloader.batch_download_from_ris(ris_file)


if __name__ == "__main__":
    main()
