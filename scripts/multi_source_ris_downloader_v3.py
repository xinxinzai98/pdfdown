#!/usr/bin/env python3
"""
RIS æ–‡ä»¶å¤šæ¸ é“æ‰¹é‡ä¸‹è½½å™¨ (å¢å¼ºç‰ˆ v3.0)

æ”¹è¿›åŠŸèƒ½:
- å¹¶å‘ä¸‹è½½ä¼˜åŒ–
- ä»£ç†æ”¯æŒï¼ˆæµ·å¤–ä»£ç† + ä¸­å›½å¤§å­¦å†…ç½‘ï¼‰
- æ›´å¤šä¸‹è½½æºï¼ˆ10+ï¼‰
- å¢å¼ºé‡è¯•æœºåˆ¶
- ç½‘é¡µå¯è§†åŒ–æŠ¥å‘Š
"""

import re
import os
import sys
import time
import json
import requests
from urllib.parse import quote, urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
from datetime import datetime


class MultiSourceDownloader:
    """å¤šæ¥æºä¸‹è½½å™¨ (å¢å¼ºç‰ˆ)"""

    def __init__(self, max_workers=3, max_retries=2):
        self.max_workers = max_workers
        self.max_retries = max_retries

        self.session = requests.Session()
        self.session.trust_env = False  # ç¦ç”¨ç³»ç»Ÿä»£ç†
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
            }
        )

        self.output_dir = "ris_downloads"
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

        self.results = {"success": [], "failed": [], "in_progress": []}
        self.lock = Lock()

        # å­˜å‚¨ DOI å…ƒæ•°æ®ï¼šå¹´ä»½ã€åˆŠç‰©ã€ç¬¬ä¸€ä½œè€…
        self.doi_metadata = {}

        self.html_report = {
            "start_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "end_time": None,
            "total": 0,
            "success": 0,
            "failed": 0,
            "items": [],
        }

    def get_proxy_config(self, use_china_network=False):
        """è·å–ä»£ç†é…ç½®

        Args:
            use_china_network: æ˜¯å¦ä½¿ç”¨ä¸­å›½å¤§å­¦å†…ç½‘ï¼ˆç»•è¿‡ä»£ç†ï¼‰

        Returns:
            proxies å­—å…¸æˆ– None
        """
        if use_china_network:
            return None
        else:
            return {"http": "http://127.0.0.1:7897", "https": "http://127.0.0.1:7897"}

    def parse_ris_metadata(self, ris_file):
        """è§£æ RIS æ–‡ä»¶ï¼Œæå– DOI çš„å…ƒæ•°æ®

        Returns:
            dict: {doi: {"year": str, "journal": str, "first_author": str}}
        """
        metadata = {}
        current_doi = None
        current_entry = {"year": "", "journal": "", "first_author": ""}

        with open(ris_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                # æ£€æŸ¥æ˜¯å¦æ˜¯å­—æ®µå¼€å§‹
                if " - " in line:
                    field, value = line.split(" - ", 1)
                    field = field.strip()
                    value = value.strip()

                    # DOI
                    if field == "DO":
                        # ä¿å­˜å‰ä¸€ä¸ªæ¡ç›®
                        if current_doi:
                            metadata[current_doi] = current_entry.copy()
                        current_doi = value
                        current_entry = {"year": "", "journal": "", "first_author": ""}

                    # å¹´ä»½ (PY)
                    elif field == "PY" and current_doi:
                        current_entry["year"] = value

                    # åˆŠç‰©åç§° (T2, J9, JI)
                    elif field in ["T2", "J9", "JI"] and current_doi:
                        if not current_entry["journal"]:
                            current_entry["journal"] = value

                    # ä½œè€… (AU)
                    elif field == "AU" and current_doi:
                        if not current_entry["first_author"]:
                            current_entry["first_author"] = value

            # ä¿å­˜æœ€åä¸€ä¸ªæ¡ç›®
            if current_doi:
                metadata[current_doi] = current_entry.copy()

        # æ¸…ç†æ•°æ®
        for doi in metadata:
            # æå–å¹´ä»½ï¼ˆå¦‚æœæ˜¯å®Œæ•´çš„æ—¥æœŸï¼Œåªå–å¹´ä»½ï¼‰
            year = metadata[doi]["year"]
            if len(year) > 4:
                year = year[:4]
            metadata[doi]["year"] = year

            # æ¸…ç†åˆŠç‰©åç§°ï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
            journal = metadata[doi]["journal"]
            journal = re.sub(r'[\\/*?:"<>|]', "", journal)
            journal = journal.strip()
            metadata[doi]["journal"] = journal

            # æ¸…ç†ä½œè€…åï¼ˆæ ¼å¼åŒ–ï¼‰
            author = metadata[doi]["first_author"]
            if author:
                # æ ¼å¼: "Last, First" æˆ– "First Last"
                if ", " in author:
                    parts = author.split(", ")
                    if len(parts) >= 1:
                        author = parts[0]
                metadata[doi]["first_author"] = author

        return metadata

    def generate_filename(self, doi, source):
        """ç”Ÿæˆæ–‡ä»¶åï¼šå¹´ä»½-åˆŠç‰©-ç¬¬ä¸€ä½œè€…-æ¥æº

        Args:
            doi: DOI
            source: ä¸‹è½½æ¥æº

        Returns:
            str: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        """
        # è·å–å…ƒæ•°æ®
        metadata = self.doi_metadata.get(doi, {})

        year = metadata.get("year", "Unknown")
        journal = metadata.get("journal", "Unknown")
        author = metadata.get("first_author", "Unknown")

        # æ¸…ç†æ–‡ä»¶åï¼ˆç§»é™¤éæ³•å­—ç¬¦ï¼‰
        safe_year = re.sub(r'[\\/*?:"<>|]', "", str(year))
        safe_journal = re.sub(r'[\\/*?:"<>|]', "", journal)
        safe_author = re.sub(r'[\\/*?:"<>|]', "", author)
        safe_source = re.sub(r'[\\/*?:"<>|]', "", source)

        # ç”Ÿæˆæ–‡ä»¶å
        filename = f"{safe_year}-{safe_journal}-{safe_author}-{safe_source}"

        return filename

    def download_doi(self, doi, index=1, total=1):
        """å°è¯•ä»å¤šä¸ªæ¥æºä¸‹è½½å•ä¸ª DOIï¼ˆæ”¯æŒé‡è¯•ï¼‰

        Args:
            doi: DOI
            index: å½“å‰ç´¢å¼•
            total: æ€»æ•°
        """
        sources = [
            ("Unpaywall API", self._try_unpaywall, False),
            ("Sci-Hub âš ï¸", self._try_scihub, True),
            ("Semantic Scholar", self._try_semantic_scholar, False),
            ("arXiv", self._try_arxiv, False),
            ("CORE", self._try_core, False),
            ("Open Access Button", self._try_openaccess, False),
            ("Europe PMC", self._try_europe_pmc, False),
            ("PubMed", self._try_pubmed, False),
            ("Paperity", self._try_paperity, False),
            ("Google Scholar", self._try_google_scholar, True),
            ("ResearchGate", self._try_researchgate, True),
        ]

        with self.lock:
            self.html_report["items"].append(
                {
                    "index": index,
                    "doi": doi,
                    "status": "processing",
                    "attempts": [],
                    "final_source": None,
                    "file": None,
                    "size": 0,
                }
            )
            item = self.html_report["items"][-1]

        retry_count = 0

        while retry_count <= self.max_retries:
            for source_name, download_func, needs_proxy in sources:
                try:
                    use_proxy = not needs_proxy
                    proxies = self.get_proxy_config(use_china_network=use_proxy)

                    with self.lock:
                        item["attempts"].append(
                            {
                                "source": source_name,
                                "retry": retry_count + 1,
                                "status": "trying",
                            }
                        )

                    print(
                        f"[{index}/{total}] [{source_name}] å°è¯• #{retry_count + 1} ...",
                        end=" ",
                    )

                    result = download_func(doi, proxies=proxies)

                    if result and result.get("success"):
                        print(f"âœ… æˆåŠŸ")

                        with self.lock:
                            item["status"] = "success"
                            item["final_source"] = source_name
                            item["file"] = result.get("file")
                            item["size"] = result.get("size", 0)
                            item["attempts"][-1]["status"] = "success"

                            self.results["success"].append(
                                {
                                    "doi": doi,
                                    "source": source_name,
                                    "file": result.get("file"),
                                    "retry": retry_count,
                                }
                            )

                        return True
                    else:
                        print(f"âŒ å¤±è´¥")

                        with self.lock:
                            item["attempts"][-1]["status"] = "failed"

                    time.sleep(0.5)

                except Exception as e:
                    print(f"âŒ é”™è¯¯: {str(e)[:50]}")

                    with self.lock:
                        item["attempts"][-1]["status"] = "error"
                        item["attempts"][-1]["error"] = str(e)

            retry_count += 1
            if retry_count <= self.max_retries:
                print(f"    ğŸ”„ é‡è¯• #{retry_count + 1}/{self.max_retries + 1}...")
                time.sleep(2)

        with self.lock:
            item["status"] = "failed"
            self.results["failed"].append(doi)

        return False

    def _try_with_fallback(self, url, timeout=10, proxies=None, allow_redirects=True):
        """ä½¿ç”¨æŒ‡å®šä»£ç†è¯·æ±‚,å¤±è´¥æ—¶å°è¯•æ— ä»£ç†"""
        try:
            return self.session.get(
                url, timeout=timeout, proxies=proxies, allow_redirects=allow_redirects
            )
        except (
            requests.exceptions.ProxyError,
            requests.exceptions.Timeout,
            requests.exceptions.SSLError,
        ) as e:
            if proxies is not None:
                print(f"    ğŸ” ä»£ç†å¤±è´¥,å°è¯•æ— ä»£ç†...")
                try:
                    return self.session.get(
                        url,
                        timeout=timeout,
                        proxies=None,
                        allow_redirects=allow_redirects,
                    )
                except Exception as e2:
                    raise e2
            raise

    def _try_unpaywall(self, doi, proxies=None):
        """å°è¯• Unpaywall API"""
        try:
            # Unpaywall API è¯·æ±‚éœ€è¦æ— ä»£ç†ï¼ˆä½¿ç”¨ session.trust_env = Falseï¼‰
            url = f"https://api.unpaywall.org/v2/{doi}?email=894643096@qq.com"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                data = response.json()
                if data.get("is_oa"):
                    pdf_url = data.get("best_oa_location", {}).get("url")
                    if pdf_url:
                        # å°è¯•å¤šç§æ–¹å¼ä¸‹è½½ PDF
                        # æ–¹æ³•1: ä½¿ç”¨é…ç½®çš„ä»£ç†
                        result = self._download_and_save(
                            pdf_url, doi, "Unpaywall", proxies
                        )
                        if result.get("success"):
                            return result

                        # æ–¹æ³•2: ä¸ä½¿ç”¨ä»£ç†
                        result = self._download_and_save(
                            pdf_url, doi, "Unpaywall", None
                        )
                        if result.get("success"):
                            return result

                        # æ–¹æ³•3: ä½¿ç”¨æµ·å¤–ä»£ç†
                        oversea_proxies = self.get_proxy_config(use_china_network=False)
                        result = self._download_and_save(
                            pdf_url, doi, "Unpaywall", oversea_proxies
                        )
                        if result.get("success"):
                            return result

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_semantic_scholar(self, doi, proxies=None):
        """å°è¯• Semantic Scholar"""
        try:
            url = f"https://api.semanticscholar.org/v1/paper/DOI:{doi}"
            response = self._try_with_fallback(url, timeout=10, proxies=proxies)

            if response.status_code == 200:
                data = response.json()

                oa_pdf = data.get("openAccessPdf")
                if oa_pdf:
                    pdf_url = oa_pdf.get("url")
                    if pdf_url:
                        return self._download_and_save(
                            pdf_url, doi, "Semantic_Scholar", proxies
                        )

                sources = data.get("sources", [])
                for source in sources:
                    url = source.get("url")
                    if url and "pdf" in url.lower():
                        return self._download_and_save(
                            url, doi, "Semantic_Scholar", proxies
                        )

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_arxiv(self, doi, proxies=None):
        """å°è¯• arXiv"""
        if "arxiv" not in doi.lower():
            return {"success": False}

        arxiv_pattern = re.compile(r"(?:10\.\d+/)?arxiv\.?/?(\d+\.\d+)", re.IGNORECASE)
        match = arxiv_pattern.search(doi)

        if match:
            arxiv_id = match.group(1)
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            return self._download_and_save(pdf_url, doi, "arXiv", proxies)

        return {"success": False}

    def _try_core(self, doi, proxies=None):
        """å°è¯• CORE"""
        try:
            url = f"https://core.ac.uk/search?q={quote(doi)}"
            response = self._try_with_fallback(url, timeout=10, proxies=proxies)

            pdf_pattern = re.compile(
                r'href=["\']([^"\']*\.pdf[^"\']*)["\']', re.IGNORECASE
            )
            pdf_links = pdf_pattern.findall(response.text)

            if pdf_links:
                for pdf_url in pdf_links[:3]:
                    if pdf_url.startswith("https://core.ac.uk/download"):
                        return self._download_and_save(pdf_url, doi, "CORE", proxies)

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_openaccess(self, doi, proxies=None):
        """å°è¯•å¼€æ”¾è·å–æŒ‰é’®"""
        try:
            url = f"https://api.openaccessbutton.org/v2/{doi}?email=your@email.com"
            response = self._try_with_fallback(url, timeout=10, proxies=proxies)

            if response.status_code == 200:
                data = response.json()
                if data.get("status") == "success" and data.get("file_type") == "pdf":
                    pdf_url = data.get("file_url")
                    if pdf_url:
                        return self._download_and_save(
                            pdf_url, doi, "OpenAccess", proxies
                        )

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_europe_pmc(self, doi, proxies=None):
        """å°è¯• Europe PMC"""
        try:
            url = f"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=DOI:{doi}&resulttype=core"
            response = self._try_with_fallback(url, timeout=10, proxies=proxies)

            if response.status_code == 200:
                pdf_pattern = re.compile(
                    r'openAccess="Y"[^>]*>([^<]+)</', re.IGNORECASE
                )
                matches = pdf_pattern.findall(response.text)

                for pdf_url in matches[:3]:
                    if pdf_url and "pdf" in pdf_url.lower():
                        return self._download_and_save(
                            pdf_url, doi, "EuropePMC", proxies
                        )

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_pubmed(self, doi, proxies=None):
        """å°è¯• PubMed (ç”Ÿç‰©åŒ»å­¦)"""
        try:
            url = f"https://pubmed.ncbi.nlm.nih.gov/?term={quote(doi)}"
            response = self._try_with_fallback(url, timeout=10, proxies=proxies)

            if response.status_code == 200:
                pdf_pattern = re.compile(
                    r'href=["\']([^"\']*\/pdf\/[^"\']*)["\']', re.IGNORECASE
                )
                pdf_links = pdf_pattern.findall(response.text)

                for pdf_url in pdf_links[:2]:
                    if "pdf" in pdf_url.lower():
                        return self._download_and_save(pdf_url, doi, "PubMed", proxies)

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_paperity(self, doi, proxies=None):
        """å°è¯• Paperity"""
        try:
            url = f"https://paperity.org/search/?q={quote(doi)}"
            response = self.session.get(url, timeout=10, proxies=proxies)

            pdf_pattern = re.compile(
                r'href=["\']([^"\']*\.pdf[^"\']*)["\']', re.IGNORECASE
            )
            pdf_links = pdf_pattern.findall(response.text)

            if pdf_links:
                for pdf_url in pdf_links[:3]:
                    if pdf_url and "download" in pdf_url.lower():
                        return self._download_and_save(
                            pdf_url, doi, "Paperity", proxies
                        )

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_google_scholar(self, doi, proxies=None):
        """å°è¯• Google Scholar (éœ€è¦ç»•è¿‡ä»£ç†)"""
        try:
            url = f"https://scholar.google.com/scholar?q={quote(doi)}"
            response = self.session.get(
                url, timeout=15, proxies=self.get_proxy_config(use_china_network=True)
            )

            if response.status_code == 200:
                pdf_pattern = re.compile(
                    r'href=["\']([^"\']*\.pdf[^"\']*)["\']', re.IGNORECASE
                )
                pdf_links = pdf_pattern.findall(response.text)

                for pdf_url in pdf_links[:2]:
                    if pdf_url and pdf_url.startswith("http"):
                        return self._download_and_save(
                            pdf_url,
                            doi,
                            "GoogleScholar",
                            self.get_proxy_config(use_china_network=True),
                        )

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_researchgate(self, doi, proxies=None):
        """å°è¯• ResearchGate (éœ€è¦ç»•è¿‡ä»£ç†)"""
        try:
            url = f"https://www.researchgate.net/search?q={quote(doi)}"
            response = self.session.get(
                url, timeout=15, proxies=self.get_proxy_config(use_china_network=True)
            )

            if response.status_code == 200:
                pdf_pattern = re.compile(
                    r'href=["\']([^"\']*\/fullText\/pdf\/[^"\']*)["\']', re.IGNORECASE
                )
                pdf_links = pdf_pattern.findall(response.text)

                for pdf_url in pdf_links[:2]:
                    if pdf_url:
                        return self._download_and_save(
                            pdf_url,
                            doi,
                            "ResearchGate",
                            self.get_proxy_config(use_china_network=True),
                        )

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _try_scihub(self, doi, proxies=None):
        """å°è¯• Sci-Hub ä¸‹è½½

        åŸºäºæ”¹è¿›ç‰ˆæµ‹è¯•å®ç°ï¼ˆä½¿ç”¨ BeautifulSoup + æ–°åŸŸåï¼‰
        """
        from bs4 import BeautifulSoup

        # ä½¿ç”¨ç»è¿‡éªŒè¯çš„åŸŸååˆ—è¡¨
        scihub_domains = [
            "https://www.sci-hub.ren",  # âœ… æ–°åŸŸå
            "https://sci-hub.hk",  # âœ… æ–°åŸŸå
            "https://sci-hub.la",  # âœ… æ–°åŸŸå
            "https://sci-hub.cat",
            "https://sci-hub.se",
            "https://sci-hub.st",
            "https://sci-hub.ru",
            "https://sci-hub.wf",
            "https://sci-hub.yt",
            "https://sci-hub.do",
            "https://sci-hub.mksa.top",
            "https://sci-hub.wf",
            "https://www.tes1e.com",  # âœ… æ–°åŸŸå
        ]

        for domain in scihub_domains:
            try:
                url = f"{domain}/{doi.replace('/', '%2F')}"

                response = self._try_with_fallback(
                    url, timeout=30, proxies=proxies, allow_redirects=True
                )

                if response.status_code != 200:
                    continue

                # ä½¿ç”¨ BeautifulSoup è§£æ HTML
                soup = BeautifulSoup(response.text, "html.parser")

                # æ–¹æ³•1: æŸ¥æ‰¾ embed æ ‡ç­¾ï¼ˆå‚è€ƒ GitHub å®ç°ï¼‰
                embed = soup.find("embed")
                if embed:
                    embed_src = str(embed.get("src", ""))
                    if embed_src and ".pdf" in embed_src:
                        print(f"    âœ… æ‰¾åˆ° embed æ ‡ç­¾ï¼Œå°è¯•ä¸‹è½½...")

                        # ç¡®ä¿ URL å®Œæ•´
                        if embed_src.startswith("//"):
                            embed_src = "https:" + embed_src
                        elif not embed_src.startswith("http"):
                            embed_src = urljoin(response.url, embed_src)

                        result = self._download_and_save(
                            embed_src, doi, "SciHub", proxies
                        )
                        if result.get("success"):
                            return result
                        else:
                            print(f"    âŒ embed æ ‡ç­¾ä¸‹è½½å¤±è´¥")

                # æ–¹æ³•2: æŸ¥æ‰¾ iframe æ ‡ç­¾
                iframe = soup.find("iframe")
                if iframe:
                    iframe_src = str(iframe.get("src", ""))
                    if iframe_src and ".pdf" in iframe_src:
                        print(f"    âœ… æ‰¾åˆ° iframe æ ‡ç­¾ï¼Œå°è¯•ä¸‹è½½...")

                        if iframe_src.startswith("//"):
                            iframe_src = "https:" + iframe_src
                        elif not iframe_src.startswith("http"):
                            iframe_src = urljoin(response.url, iframe_src)

                        result = self._download_and_save(
                            iframe_src, doi, "SciHub", proxies
                        )
                        if result.get("success"):
                            return result
                        else:
                            print(f"    âŒ iframe æ ‡ç­¾ä¸‹è½½å¤±è´¥")

                # æ–¹æ³•3: æŸ¥æ‰¾æ‰€æœ‰ PDF é“¾æ¥
                pdf_links = soup.find_all("a", href=True)
                for link in pdf_links:
                    href = str(link.get("href", ""))
                    if (
                        href
                        and ".pdf" in href.lower()
                        and "sci-hub" not in href.lower()
                    ):
                        if not href.startswith("http"):
                            href = urljoin(response.url, href)

                        result = self._download_and_save(href, doi, "SciHub", proxies)
                        if result.get("success"):
                            return result

                # æ–¹æ³•4: æ£€æŸ¥æ˜¯å¦ç›´æ¥æ˜¯ PDF å“åº”
                content_type = response.headers.get("Content-Type", "").lower()
                if "pdf" in content_type:
                    filename = f"SciHub_{doi.replace('/', '_').replace('.', '_')}.pdf"
                    filepath = os.path.join(self.output_dir, filename)

                    with open(filepath, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)

                    file_size = os.path.getsize(filepath)

                    print(f"    ğŸ“ {filename} ({file_size:,} bytes)")

                    return {"success": True, "file": filepath, "size": file_size}

            except requests.exceptions.RequestException:
                continue
            except Exception as e:
                continue

        return {"success": False}

    def _download_and_save(self, url, doi, source, proxies=None):
        """ä¸‹è½½å¹¶ä¿å­˜ PDF"""
        try:
            response = self.session.get(url, timeout=30, stream=True, proxies=proxies)

            if response.status_code == 200:
                content_type = response.headers.get("Content-Type", "").lower()

                if "pdf" in content_type or url.lower().endswith(".pdf"):
                    # ä½¿ç”¨æ–°çš„æ–‡ä»¶åç”Ÿæˆé€»è¾‘
                    filename = self.generate_filename(doi, source) + ".pdf"
                    filepath = os.path.join(self.output_dir, filename)

                    with open(filepath, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)

                    file_size = os.path.getsize(filepath)

                    print(f"    ğŸ“ {filename} ({file_size:,} bytes)")

                    return {"success": True, "file": filepath, "size": file_size}

            return {"success": False}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def batch_download_from_ris(self, ris_file):
        """ä» RIS æ–‡ä»¶æ‰¹é‡ä¸‹è½½ (å¹¶å‘)"""

        print("=" * 70)
        print("ğŸ“š RIS æ–‡ä»¶å¤šæ¸ é“æ‰¹é‡ä¸‹è½½å™¨ v3.0 (å¢å¼ºç‰ˆ)")
        print("=" * 70)
        print(f"\nğŸ“„ RIS æ–‡ä»¶: {ris_file}")

        # è§£æ RIS å…ƒæ•°æ®
        print("ğŸ“– è§£æ RIS å…ƒæ•°æ®...")
        self.doi_metadata = self.parse_ris_metadata(ris_file)
        print(f"   âœ… è§£æå®Œæˆï¼Œå…± {len(self.doi_metadata)} æ¡å…ƒæ•°æ®")

        dois = list(self.doi_metadata.keys())
        print(f"\nğŸ“‹ æ‰¾åˆ° {len(dois)} ä¸ª DOI:")
        for i, doi in enumerate(dois, 1):
            metadata = self.doi_metadata.get(doi, {})
            print(f"  [{i}] {doi}")
            print(
                f"      {metadata.get('year', 'N/A')} - {metadata.get('journal', 'N/A')} - {metadata.get('first_author', 'N/A')}"
            )

        print(f"\nğŸš€ å¼€å§‹å¹¶å‘ä¸‹è½½ (æœ€å¤§å¹¶å‘æ•°: {self.max_workers})")
        print(f"ğŸ”§ æœ€å¤§é‡è¯•æ¬¡æ•°: {self.max_retries}")
        print(f"ğŸ“ ä¿å­˜ç›®å½•: {self.output_dir}")
        print("=" * 70)

        self.html_report["total"] = len(dois)

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self.download_doi, doi, i + 1, len(dois)): (i, doi)
                for i, doi in enumerate(dois)
            }

            for future in as_completed(futures):
                idx, doi = futures[future]
                try:
                    future.result()
                except Exception as e:
                    print(f"âŒ [{idx + 1}] {doi} å‘ç”Ÿå¼‚å¸¸: {e}")
                    with self.lock:
                        self.results["failed"].append(doi)

        self.html_report["end_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.html_report["success"] = len(self.results["success"])
        self.html_report["failed"] = len(self.results["failed"])

        self.print_summary(dois)
        self.generate_html_report()

    def print_summary(self, dois):
        """æ‰“å°ä¸‹è½½æ€»ç»“"""
        print("\n" + "=" * 70)
        print("ğŸ“Š ä¸‹è½½æ€»ç»“")
        print("=" * 70)

        print(f"\nâœ… æˆåŠŸ: {len(self.results['success'])} ç¯‡")
        for item in self.results["success"]:
            print(f"   âœ“ {item['doi']}")
            print(f"     æ¥æº: {item['source']} (é‡è¯•{item.get('retry', 0)}æ¬¡)")
            print(f"     æ–‡ä»¶: {item['file']}")

        print(f"\nâŒ å¤±è´¥: {len(self.results['failed'])} ç¯‡")
        for doi in self.results["failed"]:
            print(f"   âœ— {doi}")

        success_rate = len(self.results["success"]) / len(dois) * 100
        print(f"\nğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")

        log_file = os.path.join(self.output_dir, "download_summary.txt")
        with open(log_file, "w", encoding="utf-8") as f:
            f.write(f"RIS æ–‡ä»¶æ‰¹é‡ä¸‹è½½æ€»ç»“ (v3.0 å¢å¼ºç‰ˆ)\n")
            f.write(
                f"æ—¶é—´: {self.html_report['start_time']} - {self.html_report['end_time']}\n"
            )
            f.write(f"æ€»è®¡: {len(dois)} ç¯‡\n")
            f.write(f"æˆåŠŸ: {len(self.results['success'])} ç¯‡\n")
            f.write(f"å¤±è´¥: {len(self.results['failed'])} ç¯‡\n")
            f.write(f"æˆåŠŸç‡: {success_rate:.1f}%\n")
            f.write(f"æœ€å¤§é‡è¯•æ¬¡æ•°: {self.max_retries}\n\n")

            f.write("æˆåŠŸåˆ—è¡¨:\n")
            for item in self.results["success"]:
                f.write(f"  {item['doi']}\n")
                f.write(f"    æ¥æº: {item['source']}\n")
                f.write(f"    æ–‡ä»¶: {item['file']}\n\n")

            f.write("å¤±è´¥åˆ—è¡¨:\n")
            for doi in self.results["failed"]:
                f.write(f"  {doi}\n\n")

        print(f"\nğŸ“ è¯¦ç»†æ—¥å¿—: {log_file}")

        print("\n" + "=" * 70)
        print("ğŸ’¡ æ‰‹åŠ¨ä¸‹è½½å»ºè®®")
        print("=" * 70)

        if self.results["failed"]:
            print("\nå¯¹äºæœªä¸‹è½½çš„æ–‡çŒ®ï¼Œå¯ä»¥å°è¯•ï¼š")
            print("  1. Google Scholar: https://scholar.google.com/")
            print("  2. Sci-Hub: https://sci-hub.se/ (è°¨æ…ä½¿ç”¨)")
            print("  3. ResearchGate: https://www.researchgate.net/")
            print("  4. å›¾ä¹¦é¦†èµ„æº")
            print("  5. è”ç³»ä½œè€…")

    def generate_html_report(self):
        """ç”Ÿæˆ HTML å¯è§†åŒ–æŠ¥å‘Š"""
        html_template = f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ–‡çŒ®ä¸‹è½½æŠ¥å‘Š - {self.html_report["start_time"]}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
            overflow: hidden;
        }}
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }}
        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
        }}
        .header .subtitle {{
            font-size: 1.1em;
            opacity: 0.9;
        }}
        .stats {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            padding: 30px;
            background: #f8f9fa;
        }}
        .stat-card {{
            background: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            transition: transform 0.3s;
        }}
        .stat-card:hover {{
            transform: translateY(-5px);
        }}
        .stat-value {{
            font-size: 2.5em;
            font-weight: bold;
            color: #667eea;
        }}
        .stat-label {{
            color: #6c757d;
            font-size: 0.9em;
            margin-top: 5px;
        }}
        .success .stat-value {{ color: #28a745; }}
        .failed .stat-value {{ color: #dc3545; }}
        .progress-bar {{
            height: 30px;
            background: #e9ecef;
            border-radius: 15px;
            margin: 20px 30px;
            overflow: hidden;
            display: flex;
        }}
        .progress-fill {{
            height: 100%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            transition: width 0.5s ease;
        }}
        .progress-fill.success {{
            background: linear-gradient(90deg, #28a745, #34d399);
        }}
        .progress-fill.failed {{
            background: linear-gradient(90deg, #dc3545, #f87171);
        }}
        .items {{
            padding: 30px;
        }}
        .items h2 {{
            margin-bottom: 20px;
            color: #333;
        }}
        .item {{
            background: white;
            border: 1px solid #e9ecef;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 15px;
            transition: all 0.3s;
        }}
        .item:hover {{
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
            border-color: #667eea;
        }}
        .item.success {{
            border-left: 5px solid #28a745;
        }}
        .item.failed {{
            border-left: 5px solid #dc3545;
        }}
        .item-header {{
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 10px;
        }}
        .item-doi {{
            font-weight: bold;
            font-size: 1.1em;
            color: #333;
        }}
        .item-status {{
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: bold;
            text-transform: uppercase;
        }}
        .item-status.success {{
            background: #d4edda;
            color: #155724;
        }}
        .item-status.failed {{
            background: #f8d7da;
            color: #721c24;
        }}
        .item-details {{
            font-size: 0.9em;
            color: #6c757d;
            line-height: 1.6;
        }}
        .attempt-log {{
            margin-top: 10px;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 5px;
            font-size: 0.85em;
        }}
        .attempt {{
            margin: 5px 0;
            padding: 5px;
            background: white;
            border-radius: 3px;
        }}
        .attempt.success {{
            border-left: 3px solid #28a745;
        }}
        .attempt.failed {{
            border-left: 3px solid #dc3545;
        }}
        .badge {{
            display: inline-block;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.75em;
            margin-right: 5px;
            background: #e9ecef;
        }}
        .badge.source {{
            background: #007bff;
            color: white;
        }}
        .badge.retry {{
            background: #ffc107;
            color: #333;
        }}
        .footer {{
            text-align: center;
            padding: 20px;
            background: #f8f9fa;
            color: #6c757d;
            font-size: 0.9em;
        }}
        @keyframes fadeIn {{
            from {{ opacity: 0; transform: translateY(20px); }}
            to {{ opacity: 1; transform: translateY(0); }}
        }}
        .item {{
            animation: fadeIn 0.5s ease forwards;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“š æ–‡çŒ®ä¸‹è½½æŠ¥å‘Š</h1>
            <p class="subtitle">v3.0 å¢å¼ºç‰ˆ - {self.html_report["start_time"]}</p>
        </div>

        <div class="stats">
            <div class="stat-card">
                <div class="stat-value">{self.html_report["total"]}</div>
                <div class="stat-label">æ€»æ–‡çŒ®æ•°</div>
            </div>
            <div class="stat-card success">
                <div class="stat-value">{self.html_report["success"]}</div>
                <div class="stat-label">æˆåŠŸä¸‹è½½</div>
            </div>
            <div class="stat-card failed">
                <div class="stat-value">{self.html_report["failed"]}</div>
                <div class="stat-label">ä¸‹è½½å¤±è´¥</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{self.html_report["success"] / self.html_report["total"] * 100:.1f}%</div>
                <div class="stat-label">æˆåŠŸç‡</div>
            </div>
        </div>

        <div class="progress-bar">
            <div class="progress-fill success" style="width: {self.html_report["success"] / self.html_report["total"] * 100}%">
                {self.html_report["success"]} æˆåŠŸ
            </div>
            <div class="progress-fill failed" style="width: {self.html_report["failed"] / self.html_report["total"] * 100}%">
                {self.html_report["failed"]} å¤±è´¥
            </div>
        </div>

        <div class="items">
            <h2>ğŸ“‹ ä¸‹è½½è¯¦æƒ…</h2>
"""

        for i, item in enumerate(self.html_report["items"]):
            status_class = item.get("status", "failed")
            status_text = "âœ… æˆåŠŸ" if item["status"] == "success" else "âŒ å¤±è´¥"

            html_template += f"""
            <div class="item {status_class}" style="animation-delay: {i * 0.1}s">
                <div class="item-header">
                    <span class="item-doi">[{item["index"]}] {item["doi"]}</span>
                    <span class="item-status {status_class}">{status_text}</span>
                </div>
                <div class="item-details">
"""

            if item["status"] == "success":
                html_template += f"""
                    <p><strong>ä¸‹è½½æ¥æº:</strong> <span class="badge source">{item["final_source"]}</span></p>
                    <p><strong>æ–‡ä»¶è·¯å¾„:</strong> {item["file"]}</p>
                    <p><strong>æ–‡ä»¶å¤§å°:</strong> {item["size"]:,} bytes ({item["size"] / 1024:.1f} KB)</p>
"""

            if item["attempts"]:
                html_template += f"""
                    <div class="attempt-log">
                        <strong>å°è¯•è®°å½•:</strong>
"""
                for attempt in item["attempts"]:
                    attempt_status = "âœ…" if attempt["status"] == "success" else "âŒ"
                    html_template += f"""
                        <div class="attempt {attempt["status"]}">
                            {attempt_status} <span class="badge source">{attempt["source"]}</span>
                            <span class="badge retry">é‡è¯• #{attempt["retry"]}</span>
                            {attempt["status"]}
                        </div>
"""
                html_template += """
                    </div>
"""

            html_template += """
                </div>
            </div>
"""

        html_template += f"""
        </div>

        <div class="footer">
            <p>ğŸ“… ç”Ÿæˆæ—¶é—´: {self.html_report["end_time"]}</p>
            <p>ğŸš€ ä½¿ç”¨å¤šæºå¹¶å‘ä¸‹è½½ | æœ€å¤§é‡è¯•æ¬¡æ•°: {self.max_retries} | å¹¶å‘æ•°: {self.max_workers}</p>
        </div>
    </div>
</body>
</html>
"""

        html_file = os.path.join(self.output_dir, "download_report.html")
        with open(html_file, "w", encoding="utf-8") as f:
            f.write(html_template)

        print(f"\nğŸŒ HTML å¯è§†åŒ–æŠ¥å‘Š: {html_file}")


def main():
    """ä¸»å‡½æ•°"""
    ris_file = "/Users/sanada/Desktop/20260129 åšå£«è¯¾é¢˜æ¢ç´¢/Script /04_PaperDownloader/savedrecs.ris"

    if len(sys.argv) > 1:
        ris_file = sys.argv[1]

    if not os.path.exists(ris_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {ris_file}")
        print("\nä½¿ç”¨æ–¹æ³•:")
        print("  python3 multi_source_ris_downloader_v3.py [ris_file]")
        print("\nå¯é€‰å‚æ•°:")
        print("  --workers N: è®¾ç½®å¹¶å‘æ•° (é»˜è®¤: 3)")
        print("  --retries N: è®¾ç½®é‡è¯•æ¬¡æ•° (é»˜è®¤: 2)")
        print("\nç¤ºä¾‹:")
        print("  python3 multi_source_ris_downloader_v3.py savedrecs.ris")
        print(
            "  python3 multi_source_ris_downloader_v3.py savedrecs.ris --workers 5 --retries 3"
        )
        sys.exit(1)

    max_workers = 3
    max_retries = 2

    args = sys.argv[2:]
    i = 0
    while i < len(args):
        if args[i] == "--workers" and i + 1 < len(args):
            max_workers = int(args[i + 1])
            i += 2
        elif args[i] == "--retries" and i + 1 < len(args):
            max_retries = int(args[i + 1])
            i += 2
        else:
            i += 1

    downloader = MultiSourceDownloader(max_workers=max_workers, max_retries=max_retries)
    downloader.batch_download_from_ris(ris_file)


if __name__ == "__main__":
    main()
