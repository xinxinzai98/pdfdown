#!/usr/bin/env python3
"""
Sci-Hub Playwright ä¸‹è½½å™¨
ä½¿ç”¨ Playwright ç»•è¿‡åçˆ¬è™«ä¿æŠ¤
"""

import os
import sys
import time
import re
from urllib.parse import urljoin

try:
    from playwright.sync_api import sync_playwright
except ImportError:
    print("é”™è¯¯: æœªå®‰è£… Playwright")
    print("è¯·è¿è¡Œ: pip3 install playwright")
    print("ç„¶åè¿è¡Œ: playwright install")
    sys.exit(1)


class SciHubPlaywrightDownloader:
    """ä½¿ç”¨ Playwright ä¸‹è½½ Sci-Hub æ–‡çŒ®"""

    def __init__(self, headless=True):
        """åˆå§‹åŒ– Playwright ä¸‹è½½å™¨

        Args:
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
        """
        self.headless = headless
        self.output_dir = "ris_downloads"

        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)

    def download_from_scihub(self, doi):
        """ä» Sci-Hub ä¸‹è½½æ–‡çŒ®

        Args:
            doi: DOI

        Returns:
            dict: {"success": bool, "file": str, "size": int, "error": str}
        """
        scihub_domains = [
            "https://sci-hub.ru",
            "https://sci-hub.wf",
            "https://sci-hub.mksa.top",
            "https://sci-hub.st",
            "https://sci-hub.do",
        ]

        with sync_playwright() as p:
            browser = None

            try:
                # å¯åŠ¨æµè§ˆå™¨
                browser = p.chromium.launch(headless=self.headless)

                # åˆ›å»ºæ–°é¡µé¢
                page = browser.new_page()

                # è®¾ç½® User-Agent
                page.set_extra_http_headers(
                    {
                        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                    }
                )

                for domain in scihub_domains:
                    try:
                        print(f"\nå°è¯•åŸŸå: {domain}")
                        url = f"{domain}/{doi.replace('/', '%2F')}"

                        print(f"  è®¿é—®: {url}")

                        # è®¿é—®é¡µé¢
                        page.goto(url, wait_until="dom", timeout=30000)

                        # ç­‰å¾…é¡µé¢åŠ è½½
                        print(f"  ç­‰å¾… JavaScript æ‰§è¡Œ...")
                        time.sleep(5)  # ç»™ JavaScript è¶³å¤Ÿæ—¶é—´æ‰§è¡Œ

                        # è·å–å½“å‰ URL
                        current_url = page.url
                        print(f"  å½“å‰ URL: {current_url}")

                        # è·å–é¡µé¢å†…å®¹
                        html = page.content()
                        print(f"  é¡µé¢é•¿åº¦: {len(html)} å­—ç¬¦")

                        # æ£€æŸ¥æ˜¯å¦æ˜¯ DDoS-Guard æŒ‘æˆ˜é¡µé¢
                        if "DDoS-Guard" in html:
                            print(f"  âŒ è¢« DDoS-Guard ä¿æŠ¤")
                            continue

                        # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰ PDF å†…å®¹
                        pdf_url = None

                        # æ–¹æ³•1: æŸ¥æ‰¾ PDF é“¾æ¥
                        pdf_links = self._extract_pdf_links(html, current_url)

                        if pdf_links:
                            print(f"  âœ… æ‰¾åˆ° {len(pdf_links)} ä¸ª PDF é“¾æ¥")
                            for i, link in enumerate(pdf_links[:3], 1):
                                print(f"    [{i}] {link}")

                                # å°è¯•ä¸‹è½½
                                result = self._download_pdf(
                                    link, doi, "Playwright_SciHub"
                                )

                                if result["success"]:
                                    browser.close()
                                    return result

                                time.sleep(1)

                        # æ–¹æ³•2: æŸ¥æ‰¾åµŒå…¥çš„ PDF
                        embed_pdfs = self._extract_embed_pdfs(html, domain)

                        if embed_pdfs:
                            print(f"  âœ… æ‰¾åˆ° {len(embed_pdfs)} ä¸ªåµŒå…¥ PDF")
                            for i, link in enumerate(embed_pdfs[:2], 1):
                                print(f"    [{i}] {link}")

                                result = self._download_pdf(
                                    link, doi, "Playwright_SciHub"
                                )

                                if result["success"]:
                                    browser.close()
                                    return result

                                time.sleep(1)

                        # æ–¹æ³•3: æ£€æŸ¥å½“å‰é¡µé¢æ˜¯å¦æ˜¯ PDF
                        if self._is_pdf_page(page):
                            print(f"  âœ… å½“å‰é¡µé¢æ˜¯ PDF")

                            # ä¸‹è½½ PDF
                            result = self._download_pdf(
                                current_url, doi, "Playwright_SciHub"
                            )

                            if result["success"]:
                                browser.close()
                                return result

                        print(f"  âŒ æœªæ‰¾åˆ°å¯ä¸‹è½½çš„ PDF")

                    except Exception as e:
                        print(f"  âŒ åŸŸå {domain} å¤±è´¥: {str(e)[:100]}")
                        continue

                browser.close()
                return {"success": False, "error": "æ‰€æœ‰åŸŸåå‡å¤±è´¥"}

            except Exception as e:
                if browser:
                    browser.close()
                return {"success": False, "error": str(e)}

    def _is_pdf_page(self, page):
        """æ£€æŸ¥å½“å‰é¡µé¢æ˜¯å¦æ˜¯ PDF"""
        try:
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜æˆ–å†…å®¹
            title = page.title()
            if title and "pdf" in title.lower():
                return True

            # æ£€æŸ¥å†…å®¹ç±»å‹
            content = page.content()
            if "%PDF" in content[:500]:
                return True

            return False
        except:
            return False

    def _extract_pdf_links(self, html, base_url):
        """ä» HTML ä¸­æå– PDF é“¾æ¥"""
        pdf_links = []

        # æ–¹æ³•1: href å±æ€§
        pattern = re.compile(r'href=["\']([^"\']*\.pdf[^"\']*)["\']', re.IGNORECASE)
        matches = pattern.findall(html)

        for match in matches:
            if match and match != "#" and "sci-hub" not in match.lower():
                if not match.startswith("http"):
                    match = urljoin(base_url, match)
                pdf_links.append(match)

        # æ–¹æ³•2: onclick äº‹ä»¶
        pattern2 = re.compile(
            r'onclick=["\'][^"\']*location\s*=\s*[\'"]([^"\']+\.pdf[^"\']*)["\']',
            re.IGNORECASE,
        )
        matches2 = pattern2.findall(html)

        for match in matches2:
            if match and "sci-hub" not in match.lower():
                if not match.startswith("http"):
                    match = urljoin(base_url, match)
                pdf_links.append(match)

        return list(set(pdf_links))  # å»é‡

    def _extract_embed_pdfs(self, html, base_url):
        """ä» HTML ä¸­æå–åµŒå…¥çš„ PDF"""
        embed_pdfs = []

        # æŸ¥æ‰¾ embed æ ‡ç­¾
        pattern = re.compile(r'<embed[^>]*src=["\']([^"\']+)["\']', re.IGNORECASE)
        matches = pattern.findall(html)

        for match in matches:
            if match and match.endswith(".pdf"):
                if match.startswith("//"):
                    match = "https:" + match
                elif not match.startswith("http"):
                    match = urljoin(base_url, match)
                embed_pdfs.append(match)

        return embed_pdfs

    def _download_pdf(self, url, doi, source):
        """ä¸‹è½½ PDF"""
        import requests

        proxies = {"http": "http://127.0.0.1:7897", "https": "http://127.0.0.1:7897"}

        try:
            response = requests.get(url, proxies=proxies, timeout=30, stream=True)

            if response.status_code == 200:
                content_type = response.headers.get("Content-Type", "").lower()

                if "pdf" in content_type or url.lower().endswith(".pdf"):
                    safe_doi = doi.replace("/", "_").replace(".", "_")
                    filename = f"{source}_{safe_doi}.pdf"
                    filepath = os.path.join(self.output_dir, filename)

                    with open(filepath, "wb") as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            f.write(chunk)

                    file_size = os.path.getsize(filepath)

                    print(f"    âœ… ä¸‹è½½æˆåŠŸ!")
                    print(f"       æ–‡ä»¶: {filename}")
                    print(f"       å¤§å°: {file_size:,} bytes")

                    return {"success": True, "file": filepath, "size": file_size}

            return {"success": False}

        except Exception as e:
            return {"success": False, "error": str(e)}


def main():
    """ä¸»å‡½æ•°"""
    doi = "10.3390/pr8020248"

    if len(sys.argv) > 1:
        doi = sys.argv[1]

    headless = True
    if len(sys.argv) > 2 and sys.argv[2] == "--show":
        headless = False

    print("=" * 70)
    print("ğŸ§ª Sci-Hub Playwright ä¸‹è½½å™¨")
    print("=" * 70)
    print(f"\nDOI: {doi}")
    print(f"æ— å¤´æ¨¡å¼: {'æ˜¯' if headless else 'å¦'}")

    downloader = SciHubPlaywrightDownloader(headless=headless)

    print("\nå¼€å§‹ä¸‹è½½...")
    print("=" * 70)

    result = downloader.download_from_scihub(doi)

    print("\n" + "=" * 70)
    if result["success"]:
        print("âœ… ä¸‹è½½æˆåŠŸ!")
        print(f"æ–‡ä»¶: {result['file']}")
        print(f"å¤§å°: {result['size']:,} bytes")
    else:
        print("âŒ ä¸‹è½½å¤±è´¥!")
        if "error" in result:
            print(f"é”™è¯¯: {result['error']}")

    print("=" * 70)


if __name__ == "__main__":
    main()
