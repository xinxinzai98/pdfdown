#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Wiley è®ºæ–‡ä¸‹è½½å™¨ - é€šè¿‡ CDP è¿æ¥å·²ç™»å½•çš„ Edge æµè§ˆå™¨

ä½¿ç”¨æ­¥éª¤ï¼š
1. å¯åŠ¨å¹²å‡€ Edgeï¼ˆä¸å¸¦ä»£ç†ï¼‰ï¼š
   open -na "Microsoft Edge" --args \
     --remote-debugging-port=9222 \
     --user-data-dir=/tmp/edge-cdp-profile-wiley \
     --no-proxy-server \
     --no-first-run --no-default-browser-check

2. åœ¨ Edge ä¸­æ‰‹åŠ¨ç™»å½• Wileyï¼ˆé€šè¿‡æœºæ„ç™»å½•ï¼‰

3. è¿è¡Œæ­¤è„šæœ¬ä¸‹è½½è®ºæ–‡
"""

import asyncio
import logging
import os
import re
import sys
from typing import Optional, Dict, List

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

try:
    from playwright.async_api import async_playwright

    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logger.error(
        "è¯·å®‰è£… Playwright: pip install playwright && playwright install chromium"
    )


def sanitize_filename(name: str, max_len: int = 180) -> str:
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    # æ›¿æ¢éæ³•å­—ç¬¦
    name = re.sub(r'[/\\:*?"<>|]', "_", name)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    name = re.sub(r"\s+", " ", name).strip()
    # æˆªæ–­é•¿åº¦
    if len(name) > max_len:
        name = name[:max_len]
    return name


class WileyDownloader:
    """Wiley è®ºæ–‡ä¸‹è½½å™¨ - é€šè¿‡ CDP å¤ç”¨å·²ç™»å½•æµè§ˆå™¨"""

    CDP_URL = "http://127.0.0.1:9222"
    WILEY_PDFDIRECT_TEMPLATE = (
        "https://advanced.onlinelibrary.wiley.com/doi/pdfdirect/{doi}"
    )
    WILEY_FULL_TEMPLATE = "https://advanced.onlinelibrary.wiley.com/doi/full/{doi}"

    def __init__(self, download_dir: str = "./wiley_downloads"):
        self.download_dir = download_dir
        self.browser = None
        self.context = None
        self.playwright = None

        os.makedirs(download_dir, exist_ok=True)

    async def connect(self) -> bool:
        """è¿æ¥åˆ°å·²æ‰“å¼€çš„ Edge æµè§ˆå™¨"""
        if not PLAYWRIGHT_AVAILABLE:
            logger.error("Playwright æœªå®‰è£…")
            return False

        try:
            self.playwright = await async_playwright().start()

            logger.info(f"æ­£åœ¨è¿æ¥ Edge æµè§ˆå™¨: {self.CDP_URL}")
            self.browser = await self.playwright.chromium.connect_over_cdp(self.CDP_URL)

            # è·å–ç°æœ‰ contextï¼ˆåŒ…å«ç™»å½• cookieï¼‰
            contexts = self.browser.contexts
            if contexts:
                self.context = contexts[0]
                logger.info(f"âœ… å·²è¿æ¥åˆ° Edgeï¼Œå¤ç”¨ç°æœ‰ç™»å½•ä¸Šä¸‹æ–‡")
                return True
            else:
                logger.error("æœªæ‰¾åˆ°æµè§ˆå™¨ä¸Šä¸‹æ–‡")
                return False

        except Exception as e:
            logger.error(f"è¿æ¥å¤±è´¥: {e}")
            logger.error("è¯·ç¡®ä¿ Edge å·²å¯åŠ¨å¹¶å¼€å¯è°ƒè¯•ç«¯å£:")
            logger.error(
                '  open -na "Microsoft Edge" --args --remote-debugging-port=9222 --user-data-dir=/tmp/edge-cdp-profile-wiley --no-proxy-server'
            )
            return False

    async def close(self):
        """æ–­å¼€è¿æ¥ï¼ˆä¸å…³é—­æµè§ˆå™¨ï¼‰"""
        # æ³¨æ„ï¼šä¸å…³é—­ browserï¼Œåªæ–­å¼€è¿æ¥
        self.context = None
        self.browser = None
        if self.playwright:
            await self.playwright.stop()
            self.playwright = None

    async def download_wiley(
        self, doi: str, metadata: Optional[Dict] = None
    ) -> Optional[str]:
        """ä» Wiley ä¸‹è½½ PDF - ä½¿ç”¨ç½‘ç»œæ‹¦æˆªè·å–å®é™… PDF å“åº”"""

        if not self.context:
            logger.error("æœªè¿æ¥åˆ°æµè§ˆå™¨ï¼Œè¯·å…ˆè°ƒç”¨ connect()")
            return None

        pdf_url = self.WILEY_PDFDIRECT_TEMPLATE.format(doi=doi)
        pdf_data_holder = {"data": None}

        pages = self.context.pages
        if pages:
            page = pages[-1]
        else:
            page = await self.context.new_page()

        async def capture_pdf_response(route, request):
            nonlocal pdf_data_holder
            response = await route.fetch()
            body = await response.body()

            content_type = response.headers.get("content-type", "")
            if "pdf" in content_type.lower() or body[:4] == b"%PDF":
                logger.info(f"[Wiley] æ‹¦æˆªåˆ° PDF å“åº”: {len(body):,} bytes")
                pdf_data_holder["data"] = body

            await route.fulfill(response=response)

        try:
            logger.info(f"[Wiley] ä¸‹è½½ PDF: {pdf_url}")

            await page.route("**/*", capture_pdf_response)

            response = await page.goto(pdf_url, timeout=60000)

            if not response:
                logger.error("[Wiley] æ— å“åº”")
                return None

            logger.info(f"[Wiley] Response status: {response.status}")

            await asyncio.sleep(2)
            await page.wait_for_load_state("networkidle", timeout=30000)

            pdf_data = pdf_data_holder["data"]

            if not pdf_data:
                initial_body = await response.body()
                if initial_body[:4] == b"%PDF":
                    pdf_data = initial_body
                    logger.info(f"[Wiley] ä»åˆå§‹å“åº”è·å– PDF: {len(pdf_data):,} bytes")

            if not pdf_data:
                logger.info("[Wiley] å°è¯•ç­‰å¾… embed åŠ è½½...")
                for i in range(10):
                    await asyncio.sleep(1)
                    if pdf_data_holder["data"]:
                        pdf_data = pdf_data_holder["data"]
                        break

            if not pdf_data:
                await page.unroute("**/*", capture_pdf_response)
                await page.route("**/*.pdf**", capture_pdf_response)

                logger.info("[Wiley] å°è¯•ç›´æ¥è®¿é—® PDF URL...")
                pdf_response = await page.goto(
                    pdf_url, wait_until="networkidle", timeout=60000
                )
                if pdf_response:
                    body = await pdf_response.body()
                    if body[:4] == b"%PDF":
                        pdf_data = body

            if not pdf_data:
                embed = await page.query_selector("embed[type='application/pdf']")
                if embed:
                    src = await embed.get_attribute("src")
                    logger.info(f"[Wiley] embed src: {src}")

                    if src and src != "about:blank":
                        if src.startswith("//"):
                            src = "https:" + src
                        logger.info(f"[Wiley] å°è¯•ä» embed src è·å–: {src[:80]}...")
                        try:
                            pdf_response = await self.context.request.get(src)
                            if pdf_response.status == 200:
                                pdf_data = await pdf_response.body()
                                if pdf_data[:4] == b"%PDF":
                                    logger.info(
                                        f"[Wiley] ä» embed è·å–æˆåŠŸ: {len(pdf_data):,} bytes"
                                    )
                        except Exception as e:
                            logger.warning(f"[Wiley] embed è·å–å¤±è´¥: {e}")

            try:
                await page.unroute("**/*", capture_pdf_response)
            except:
                pass

            if not pdf_data or pdf_data[:4] != b"%PDF":
                logger.error(
                    f"[Wiley] æ— æ³•è·å–æœ‰æ•ˆ PDF (size={len(pdf_data) if pdf_data else 0})"
                )
                return None

            logger.info(f"[Wiley] è·å–åˆ°æœ‰æ•ˆ PDF: {len(pdf_data):,} bytes")

            if metadata:
                author = metadata.get("first_author", "Unknown")
                year = metadata.get("year", "")
                title = metadata.get("title", "Untitled")[:50]
                doi_safe = doi.replace("/", "_")
                filename = f"{author}_{year}_{title}_{doi_safe}.pdf"
                filename = sanitize_filename(filename)
            else:
                filename = f"wiley_{doi.replace('/', '_')}.pdf"

            filepath = os.path.join(self.download_dir, filename)
            with open(filepath, "wb") as f:
                f.write(pdf_data)
            logger.info(f"âœ… [Wiley] ä¸‹è½½æˆåŠŸ: {filepath}")
            return filepath

        except Exception as e:
            logger.error(f"[Wiley] ä¸‹è½½å¤±è´¥: {e}")
            import traceback

            traceback.print_exc()
            return None

    async def batch_download(
        self, dois: List[str], metadata_list: Optional[List[Dict]] = None
    ) -> Dict:
        """æ‰¹é‡ä¸‹è½½

        Args:
            dois: DOI åˆ—è¡¨
            metadata_list: å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            ä¸‹è½½ç»“æœç»Ÿè®¡
        """
        results = {
            "total": len(dois),
            "success": 0,
            "failed": 0,
            "files": [],
            "errors": [],
        }

        for i, doi in enumerate(dois, 1):
            logger.info(f"\n{'=' * 60}")
            logger.info(f"[{i}/{len(dois)}] DOI: {doi}")
            logger.info(f"{'=' * 60}")

            metadata = (
                metadata_list[i - 1]
                if metadata_list and i <= len(metadata_list)
                else None
            )

            filepath = await self.download_wiley(doi, metadata)

            if filepath:
                results["success"] += 1
                results["files"].append({"doi": doi, "file": filepath})
            else:
                results["failed"] += 1
                results["errors"].append(doi)

            # é¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(1)

        return results


class SciHubDownloader:
    """Sci-Hub ä¸‹è½½å™¨ - éœ€è¦èµ°ä»£ç†"""

    def __init__(
        self,
        proxy: str = "http://127.0.0.1:7897",
        download_dir: str = "./scihub_downloads",
    ):
        self.proxy = proxy
        self.download_dir = download_dir
        self.browser = None
        self.context = None
        self.playwright = None

        os.makedirs(download_dir, exist_ok=True)

    async def init(self):
        """åˆå§‹åŒ–æµè§ˆå™¨ï¼ˆå¸¦ä»£ç†ï¼‰"""
        if not PLAYWRIGHT_AVAILABLE:
            return False

        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=False,  # Sci-Hub å¯èƒ½éœ€è¦æ‰‹åŠ¨éªŒè¯
            proxy={"server": self.proxy},
        )
        self.context = await self.browser.new_context()
        logger.info(f"[Sci-Hub] æµè§ˆå™¨å·²å¯åŠ¨ï¼ˆä»£ç†: {self.proxy}ï¼‰")
        return True

    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.context:
            await self.context.close()
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()

    async def download(self, doi: str, wait_time: int = 30) -> Optional[str]:
        """ä» Sci-Hub ä¸‹è½½"""
        if not self.context:
            await self.init()

        page = await self.context.new_page()

        mirrors = [
            "https://sci-hub.se",
            "https://sci-hub.st",
            "https://sci-hub.ru",
            "https://sci-hub.do",
        ]

        try:
            for mirror in mirrors:
                url = f"{mirror}/{doi}"
                logger.info(f"[Sci-Hub] å°è¯•: {url}")

                try:
                    await page.goto(url, timeout=30000)

                    # ç­‰å¾…é¡µé¢åŠ è½½
                    for i in range(wait_time):
                        await asyncio.sleep(1)

                        # æŸ¥æ‰¾ embed
                        embed = await page.query_selector("embed[src]")
                        if embed:
                            src = await embed.get_attribute("src")
                            if src and ("pdf" in src.lower() or src.startswith("http")):
                                if src.startswith("//"):
                                    src = "https:" + src

                                # ä¸‹è½½ PDF
                                logger.info(f"[Sci-Hub] æ‰¾åˆ° PDF: {src[:80]}...")
                                response = await self.context.request.get(src)

                                if response.status == 200:
                                    pdf_data = await response.body()
                                    filename = f"scihub_{doi.replace('/', '_')}.pdf"
                                    filepath = os.path.join(self.download_dir, filename)

                                    with open(filepath, "wb") as f:
                                        f.write(pdf_data)

                                    logger.info(f"âœ… [Sci-Hub] ä¸‹è½½æˆåŠŸ: {filepath}")
                                    return filepath

                        if i % 5 == 0:
                            logger.info(f"[Sci-Hub] ç­‰å¾…ä¸­... ({i}/{wait_time}s)")

                except Exception as e:
                    logger.debug(f"[Sci-Hub] {mirror} å¤±è´¥: {str(e)[:30]}")

            logger.warning(f"[Sci-Hub] æ‰€æœ‰é•œåƒå‡å¤±è´¥: {doi}")
            return None

        finally:
            await page.close()


def parse_ris_file(ris_path: str) -> List[Dict]:
    """è§£æ RIS æ–‡ä»¶"""
    papers = []
    current = {}

    with open(ris_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            if line.startswith("TY  -"):
                if current.get("doi"):
                    papers.append(current)
                current = {}
            elif line.startswith("DO  -"):
                current["doi"] = line[5:].strip()
            elif line.startswith("TI  -"):
                current["title"] = line[5:].strip()
            elif line.startswith("AU  -"):
                if "authors" not in current:
                    current["authors"] = []
                current["authors"].append(line[5:].strip())
            elif line.startswith("PY  -"):
                current["year"] = line[5:].strip()[:4]

        if current.get("doi"):
            papers.append(current)

    # æå–ç¬¬ä¸€ä½œè€…
    for paper in papers:
        if paper.get("authors"):
            paper["first_author"] = paper["authors"][0].split(",")[0]
        else:
            paper["first_author"] = "Unknown"

    return papers


async def main():
    import argparse

    parser = argparse.ArgumentParser(description="Wiley/Sci-Hub è®ºæ–‡ä¸‹è½½å™¨")
    parser.add_argument("input", help="DOI æˆ– RIS æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--source", choices=["wiley", "scihub"], default="wiley", help="ä¸‹è½½æº"
    )
    parser.add_argument("--output", "-o", default="./downloads", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--proxy", default="http://127.0.0.1:7897", help="Sci-Hub ä»£ç†")
    parser.add_argument("--wait", type=int, default=30, help="Sci-Hub ç­‰å¾…æ—¶é—´")

    args = parser.parse_args()

    # åˆ¤æ–­æ˜¯ DOI è¿˜æ˜¯æ–‡ä»¶
    if os.path.exists(args.input):
        papers = parse_ris_file(args.input)
        dois = [p["doi"] for p in papers]
        logger.info(f"ğŸ“‹ ä» RIS æ–‡ä»¶è§£æåˆ° {len(dois)} ç¯‡è®ºæ–‡")
    else:
        dois = [args.input]
        papers = [{"doi": args.input}]

    if args.source == "wiley":
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     Wiley è®ºæ–‡ä¸‹è½½å™¨ (CDP æ¨¡å¼)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  è¯·ç¡®ä¿ Edge æµè§ˆå™¨å·²å¯åŠ¨å¹¶ç™»å½• Wiley:             â•‘
â•‘                                                    â•‘
â•‘  open -na "Microsoft Edge" --args \\               â•‘
â•‘    --remote-debugging-port=9222 \\                  â•‘
â•‘    --user-data-dir=/tmp/edge-cdp-profile-wiley \\   â•‘
â•‘    --no-proxy-server \\                             â•‘
â•‘    --no-first-run --no-default-browser-check       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

        downloader = WileyDownloader(download_dir=args.output)

        if not await downloader.connect():
            sys.exit(1)

        try:
            results = await downloader.batch_download(dois, papers)

            print(f"\n{'=' * 60}")
            print(f"ğŸ“Š ä¸‹è½½å®Œæˆ")
            print(f"{'=' * 60}")
            print(f"âœ… æˆåŠŸ: {results['success']}/{results['total']}")
            print(f"âŒ å¤±è´¥: {results['failed']}/{results['total']}")

            if results["files"]:
                print("\næˆåŠŸåˆ—è¡¨:")
                for item in results["files"]:
                    print(f"  âœ“ {item['doi']}")
                    print(f"    {item['file']}")

        finally:
            await downloader.close()

    elif args.source == "scihub":
        print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     Sci-Hub è®ºæ–‡ä¸‹è½½å™¨                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ä»£ç†: {args.proxy:<42}â•‘
â•‘  ç­‰å¾…æ—¶é—´: {args.wait}s                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

        downloader = SciHubDownloader(proxy=args.proxy, download_dir=args.output)

        try:
            for doi in dois:
                await downloader.download(doi, wait_time=args.wait)
        finally:
            await downloader.close()


if __name__ == "__main__":
    asyncio.run(main())
